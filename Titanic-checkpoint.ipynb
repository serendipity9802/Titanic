{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 经典又兼具备趣味性的Kaggle案例[泰坦尼克号问题](https://www.kaggle.com/c/titanic)\n",
    "大家都熟悉的『Jack and Rose』的故事，豪华游艇倒了，大家都惊恐逃生，可是救生艇的数量有限，无法人人都有，副船长发话了『lady and kid first！』，所以是否获救其实并非随机，而是基于一些背景有rank先后的。<br>\n",
    "训练和测试数据是一些乘客的个人信息以及存活状况，要尝试根据它生成合适的模型并预测其他人的存活状况。<br>\n",
    "对，这是一个二分类问题，很多分类算法都可以解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>看看数据长什么样</font>**<br>\n",
    "还是用pandas加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个ipython notebook主要是我解决Kaggle Titanic问题的思路和过程\n",
    "\n",
    "import pandas as pd #数据分析\n",
    "import numpy as np #科学计算\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "data_train = pd.read_csv(\"Train.csv\")\n",
    "data_train.columns\n",
    "#data_train[data_train.Cabin.notnull()]['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>我们看大概有以下这些字段</font>**<br>\n",
    "PassengerId => 乘客ID<br>\n",
    "Pclass => 乘客等级(1/2/3等舱位)<br>\n",
    "Name => 乘客姓名<br>\n",
    "Sex => 性别<br>\n",
    "Age => 年龄<br>\n",
    "SibSp => 堂兄弟/妹个数<br>\n",
    "Parch => 父母与小孩个数<br>\n",
    "Ticket => 船票信息<br>\n",
    "Fare => 票价<br>\n",
    "Cabin => 客舱<br>\n",
    "Embarked => 登船港口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>我这么懒的人显然会让pandas自己先告诉我们一些信息<font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>上面的数据说啥了？它告诉我们，训练数据中总共有891名乘客，但是很不幸，我们有些属性的数据不全，比如说：<font><br>\n",
    "\n",
    "* <font color=red>Age（年龄）属性只有714名乘客有记录<font>\n",
    "* <font color=red>Cabin（客舱）更是只有204名乘客是已知的<font>\n",
    "\n",
    "<font color=red>似乎信息略少啊，想再瞄一眼具体数据数值情况呢？恩，我们用下列的方法，得到数值型数据的一些分布(因为有些属性，比如姓名，是文本型；而另外一些属性，比如登船港口，是类目型。这些我们用下面的函数是看不到的)<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>mean字段告诉我们，大概0.383838的人最后获救了，2/3等舱的人数比1等舱要多，平均乘客年龄大概是29.7岁(计算这个时候会略掉无记录的)等等…<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=red>『对数据的认识太重要了！』<font>\n",
    "* <font color=red>『对数据的认识太重要了！』<font>\n",
    "* <font color=red>『对数据的认识太重要了！』<font>\n",
    "\n",
    "<font color=red>口号喊完了，上面的简单描述信息并没有什么卵用啊，咱们得再细一点分析下数据啊。<font><br>\n",
    "<font color=red>看看**每个/多个 属性和最后的Survived**之间有着什么样的关系<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd81PX9wPHX+y4TCCSsGMIIm0BBRhgqWFBBEBUUFJWK\nA6WuFqs/q7Z1dohWKzgrVStSFa0LxImMqsEgUzYSMEpCWIEEAgkkl8/vj+834RLusriV5P3kkcfd\nfT7f8bnF+76fKcYYlFJKqco4gl0ApZRSoU+DhVJKqSppsFBKKVUlDRZKKaWqpMFCKaVUlTRYKKWU\nqpIGixAiIm+JyPggnj9SRLaKSKtglUGphqAufteloY+zEJFxwD0esj4B/gO86SEv2xhzhYjMB1p4\nyJ8I3AJc4CHvr8aYTz2Uow8wD+hljDEikgC8BKQACUBHY0yGve1g4CkPx14L3A0s8ZCHMWaoiLwE\n9PKQ/RtjzFoR+T0Qb4y529MxGoJKPhNfAKM8pJ/W5wGI8HK+Kj+DHtLrhGB87ziN11m/6xBWnY3q\nuQTgYWPMl6UJItIEeA5oBCwzxvzJfQcRede+W2SMGVoh70kgCugBDDfGFLvlXQzEeynHr4E3zMno\nXQJ8BjwGLK+wbRzwmjHmZQ/lcgAZxphfeSlzCw9lvgNoZj98E1gnIn8wxhz3Utb6zttn4mX883mI\n8nK+6nwG66pgfO9O53Vu8N91rYYKHWOA/5U+MMbsNca8AKwMZCGMMZnAIWBIIM+rVANSJ7/rGixC\ngIg0BjoC24JdFtsW4MxgF0Kp+qYuf9c1WISGWPv2SFBLcdIRTpZJKeU7dfa7rsEiNOTatzFBLcVJ\nMZwsk1LKd+rsd12DRQgwxhwFdgDdgl0WWzLwfbALoVR9U5e/6xosQscnwC/dE0QkCoi0H0baj/1K\nRBKB5kCav8+lVANVJ7/rGixCx2xgsoiIW1oBkG/f32o/9rdrgDkNuNusUv5WJ7/rGixChDFmI9bl\n4Di3NKn4588yiEgkMBWY4c/zKNWQ1dXvug7KszwlIofcHjux6hUBrhWRoRW2Lx3J2VtEllXI64w1\nyAdgsYi4D5FvgefRmAAYY66pQZnvEZFfVUgrsm9HeihX6UjOVh7yEoGb7V8YPWpQhvrM02ciC/99\nHmr7GazLgvG90+96Lb/rDX66D6WUUlXTaiillFJV0mChlFKqSvWmzaJly5YmKSkp2MVQwOrVqw8Y\nY3wyzbm+r6HDl+8r6HsbKqr7vtabYJGUlMSqVauCXQwFiMhPvjqWvq+hw5fvK+h7Gyqq+77Wm2Ch\nlFKqvA/XZvH3z7exO7eANrHR3HNhd8b3S6zVsTRYKKVUPfTh2izuf38DBUUuALJyC7j//Q0AtQoY\n2sCtlFL10N8/31YWKEoVFLn4++e1mx29QV9ZJN33sV+PnzFjrF+Pryyn+z7q+6Tqo925nmcM8ZZe\nFb2yUEqpeqhNbHSN0quiwUIpVW0i0l1E1rn9HRaRO0XkYRHJcku/KNhlbejuubA70eHOcmnR4U7u\nubB7rY7XoKuhlFI1Y4zZBvQFEJHS+bI+AG4AnjbGPBnE4ik3pY3Y2htKKRVs5wM7jDE/lZ9tu/Z8\n2dVTWQHDV6+fVkMppWrrKuAtt8d3iMh6EXlVROJqerDSrp5ZuQUYTnb1/HBtls8KrGpPryyUUjUm\nIhHApcD9dtKLwJ8BY98+BdzoYb9pwDSA+Ph4li1bVpa3d88RbutRUmGPYvZuW8OyvO2+fgqqhjRY\nKKVqYwywxhizF6D0FkBE/gUs9LSTMWY21kpxpKSkmOHDh5fl3XDfxxgPlR0C/Dhj+CnpKrC0Gkop\nVRtX41YFJSIJbnmXARtresBGEc4apavA0isLpVSNiEhjYCTwa7fkJ0SkL1Y1VEaFvGo5esJVo3QV\nWBoslFI1Yow5SoVlXY0x1wapOCpAtBpKKaVUlTRYKKWUqpIGC6VUSDinc/MapavA0mChTpvL5aJf\nv35cfPHFpUkRIrJCRNJF5G27Tz4iEmk/Trfzk4JVZhV6rkhpX6N0FVgaLNRpmzVrFsnJye5JbbHm\nCeoCHAKm2ulTgUN2+tPA4wEtqApp9/x3XY3SVWBpsFCnJTMzk48//pibbroJAGMMQAzwrr3JHGC8\nfX+c/Rg7/3zx1aRCqs4rqjh4u4p0VbU/fbiBzvd/QtJ9H9P5/k/404cban0sDRbqtNx555088cQT\nOBzWRyknJwfAZYwptjfJBEpnMksEdgHY+XlU6IKplPKNP324gf+k/YzL+gGHyxj+k/ZzrQOGBgtV\nawsXLqR169YMGDDAp8cVkWkiskpEVu3fv9+nx1aqofhP2s81Sq+KDspTtZaamsqCBQv45JNPKCws\n5PDhw0yfPh3AKSJh9tVDW6w1D7Bv2wGZIhIGNANyKh634vxBgXguSqnK6ZWFqrXHHnuMzMxMMjIy\nmDdvHueddx5vvPEGwBFgor3ZdcB8+/4C+zF2/hJjN3IopUKbBgvlD5nAXSKSjtUm8Yqd/grQwk6/\nC7gvSOVTStVQQKuh7GUYVwFZxpiLRaQjMA/rP5TVwLXGmBMiEgm8DgzAqqaYZIzJCGRZVc0MHz4c\nt+mmTxhjBlXcxhhTCFwRyHIppXwj0FcW04Etbo8fR/vjK6VUyAtYsBCRtsBY4GX7sQDnof3xlVIq\n5AXyymIm8HugdIhNCyBX++MrpVToC0iwEJGLgX3GmNU+Pq72x1dKqQAI1JXFOcClIpKB1aB9HjAL\niLX724Pn/vhU1R/fGJNijElp1aqVf5+BUko1YAEJFsaY+40xbY0xScBVWP3rJwNL0f74SikV8oI9\nzuJetD++UkqFvIBP92GMWQYss+/vBLQ/vlJKhTidG0opVSN22+MRwAUUG2NSRKQ58DaQBGQAVxpj\nDgWrjMr3gl0NpZSqm0YYY/oaY1Lsx/cBi40xXYHFaNVxvaPBQinlC+4Dad0H2Kp6QquhlFI1ZYAv\nRMQAL9lTyscbY7Lt/D1AvKcdRWQaMA0gPj6eZcuWleXd3bvY0y4A5bZT1ePr11ODhVKqpoYaY7JE\npDWwSES2umcaY4wdSE5Rca0St8knuf6+j72eMGPycK95yjNfv54aLOq4pEo+EL6QMWOsX4+v6h5j\nTJZ9u09EPsDq0bhXRBKMMdkikgDsC2ohlc9pm4VSqtpEpLGIxJTeB0YBGyk/kNZ9gK2qJ/TKQilV\nE/HAB/Yk0GHAm8aYz0RkJfCOiEwFfgKuDGIZlR9osFBKVZs9kPZMD+k5wPmBL5EKFK2GUkopVSUN\nFkoppapU42ooEXmwik32GWP+WcvyqCDJTX3LY/qjj1pLkLRu3ZpbbrklkEVSfvToo49Wmq/vt6qo\nNm0WQ7CmGfe2zOkcQINFHXN891ZaXXov1nirk6ZPvxCA6667Tv/zqEfS0tKYN28e3mb+1/dbVVSb\nYOEyxhz2lultMI4KbSIOHJGNTklv1qyZna9LoNcnTqeTpk2bes3X91tVVJs2i6qCgQaLuqiK/xz0\nP4/6par3U99vVVFtrizCRcTbTxIBnKdRHhUkxuWi5PixU9IPHz6MMQaXyxWEUil/KSoq4vBhzxUE\n+n4rT2oTLNKAO73kCfBp7YujgiWyTXcOrzp10O3MmVswxjBmzJgglEr5y5AhQ5g5c6bHPH2/lSe1\nCRaD0Qbueud49jZt4G5AVqxYoQ3cqka0gVsB2sDd0GgDt6opbeBWFm3wbFC0gVvVlDZwK0AbuBsa\nbeBWNXU6Ddzefnp8VvviqGApa+CuUIf99NObARg9enQwiqX8pLSB21ubhb7fqqIaBwtjzCP+KIgK\nrtih13hMf+gh74sf7dq1iylTprB3715EhGnTpjF9+nQAp4gsApKADOBKY8whseo2ZgEXAceA640x\na3z6RILkdBehCvQiUw899FBAz6fqPp1IUNVaWFgYTz31FJs3byYtLY3nn3+ezZs3AyQAi40xXYHF\nwH32LmOArvbfNODFYJRbKVVzGixUrSUkJNC/f38AYmJiSE5OJisrCyAWqws19u14+/444HVjSQNi\n7SU4lVIhToOF8omMjAzWrl3L4MGDAcKMMdl21h6s1dUAEoFdbrtl2mnliMg0EVklIqv279/vz2Ir\npaopIMFCRNqJyFIR2Swim0Rkup3eXEQWich2+zbOThcReUZE0kVkvYj0D0Q5Ve3k5+czYcIEZs6c\neUrffWO1oNaoO7UxZrYxJsUYk9KqVStfFlUpVUuBurIoBu42xvTEmuL8dhHpiVWXrXXbdVhRURET\nJkxg8uTJXH755aXJxaXVS/btPjs9C2jntntbO00pFeICsga3XSWRbd8/IiJbsKofxgHD7c3mAMuA\ne3Gr2wbSRCRWRBLcqjZUCDDGMHXqVJKTk7nrrrvcs3KB64AZ9m3ppFMLgDtEZB7WtDF5+p76xun2\nxoLq9cgSkXbA61hViwaYbYyZJSIPAzcDpfWGfzDGfHLahVIhIyDBwp2IJAH9gBVAfA3rtsv9xyIi\n07CuPGjfvr3fyqw8S01NZe7cufTu3Zu+ffsC8Le//Q2s92mkiEwFfgKutHf5BKvbbDpW19kbAl5o\ndbpKawnWiEgMsNruJg3wtDHmySCWTflRQIOFiDQB3gPuNMYcdp9SwBhjajqvlDFmNjAbICUlRacZ\nCbChQ4d6G9TlMsacXzHRvlK83e8FU35TSS2BqucCFixEJBwrULxhjHnfTt5bWr2kddtK1S0VagnO\nwapinAKswrr6OORhn7LagPj4eJYtW1aWd3fvYq/nct9OVY+vX8+ABAt75O4rwBZjzD/cshagddtK\n1TkeagleBP6M1Y7xZ+Ap4MaK+1WsDRg+fHhZ3vWVtLtkTB7uNU955uvXM1BXFucA1wIbRGSdnfYH\nrCDxjtZtK1V3eKolMMbsdcv/F7AwSMVTfhKo3lDf4H3iQa3bVqqO8FZLUKG34mXAxmCUT/lPwHtD\nKaXqNG+1BFeLSF+saqgM4NfBKZ7yFw0WSqlqq6SWQMdU1HM6N5RSSqkqabBQSilVJQ0WSimlqqTB\nQimlVJU0WCillKqSBgullFJV0mChlFKqShoslFJKVUmDhVIqJEQ5Pc8I5C1dBZYGC6VUSCh0eV6S\nxlu6CiwNFkoppaqkwUIppVSVNFgopZSqkgYLpZRSVdJgoZRSqkoaLJRSSlVJg4VSKiR4G02hoyxC\ngwYLpVRI8DaaQkdZhAYNFkoppaqkwUIp5RMiMlpEtolIuojcF+zyKN/SYKGUOm0i4gSeB8YAPYGr\nRaRncEulfEmDhVLKFwYB6caYncaYE8A8YFyQy6R8KCzYBVBK1QuJwC63x5nA4Iobicg0YBpAfHw8\ny5YtK8u7u3ex14O7b6eqx9evZ0gHCxEZDcwCnMDLxpgZQS6S8gF9XxsuY8xsYDZASkqKGT58eFne\nlx9u4D9pP5+yz6+GtOc3w3sHqoj1xvX3few1L2Py8BofL2SrobQOtH7S97XeygLauT1ua6dV21/G\n9+ZXQ9rjFGtkhVOEXw1pz1/Ga6CojZmT+tYovSqhfGVRVgcKICKldaCbg1oqdbr0fa2fVgJdRaQj\nVpC4Crimpgf5y/jeGhx8ZHy/RAD+/vk2ducW0CY2mnsu7F6WXlNiTGgOeRGRicBoY8xN9uNrgcHG\nmDvctimr/wS6A9v8XKyWwAE/n8PfAvEcOhhjWnnKCNL7GgrvW30og9f3FUBELgJmYlUvvmqM+Wtl\nBxOR/cBPXrJD4fWqjvpQzkrf11KhfGVRJff6z0AQkVXGmJRAnc8f6sJz8PX7GgrPuSGUwRjzCfBJ\nDbavLPAE/fWqjoZUzpBts8AHdaAqJOn7qlQdFMrBoqwOVEQisOpAFwS5TOr06fuqVB0UstVQxphi\nEbkD+JyTdaCbglysgFV5+VFQn0OQ3tdQeN+0DDVTV8raYMoZsg3cSimlQkfIXlko3xORccA9HrK+\nAEZ5SM82xlwhIvOBFh7yJwK3ABd4yPsrEOHlfJ8A/wHeDKVzekhXIa6hf6aNMX/zkO4XGiwalgTg\nYWPMl6UJItIEeBlYZoz5k/vGIvKufbfIGDO0Qt6TQBTQAxhujCl2y7sYiLfzPZ3vOaBRCJ5T1T0N\n/TMdMKHcwK2UqseqmtJcRCJF5G07f4WIJAW+lGX6A48Bj2J1yqioC7AcuElE1onITYEsHICIvCoi\n+0Rko5d8EZFn7NdzvYj0r8nx9cqiEiLSA2t0cemQxyxggTFmS/BKpaqhu4jkGWNW2lOJXMrJ97BB\nsD+7icAKY0y+W/poY8xnwStZWTlKp30ZiTXp4EoRWWCMcR/JPxU4ZIzpIiJXAY8Dk4JU1luBfwDH\ngLtEpGeFsoJVFZVR8coigF7Dutp43Uv+GKCr/TcYeBEPkz16o1cWXojIvVjTLAvwnf0nwFv1YWEX\nEbkh2GXwk7HA7cCLIvIY1penMdAb+HUwCwaBed1F5LfAfOA3wEa7Xr9UwOq4q1CdKc3HAXPs++8C\n54tIMJbkHgRkA/sBF7CUEJx+3RjzFXCwkk3GAa8bSxoQKyIJ1T2+Xll4NxXoZYwpck8UkX8Am4C6\nPlPqI8C/g10IP+gP3AB8A+zBGvRXAiQBFwH/DVrJLIF43W8GBhhj8u2qm3dFJMkYMwvrB08oqM6U\n5mXb2F2u87AaiAM9vUYiVqAodQDPV6qjAGNf1f3OGLPLwzbB5Ok1T8QKhFXSYOFdCdCGU+euSbDz\nQp6IrK+Q1Bw4ISJHsRrO6iMXUGKMOSYiO4wxh+3GQBcBet88vO5lWQTmdXeUVj0ZYzJEZDhWwOhA\n6ASL+mYXVlXVJPv+HOC8oJbIxzRYeHcnsFhEtnMyGrfHasi6w+teoSUeuBA4ZD+eDGRgNcQtD1KZ\n/M0FRNr3B7ilhxO4IF/xdS8lBOZ13ysifY0x6wDsK4yLgVexquNCQXWmfSndJlNEwoBmQE5gindK\nOdznsWoJ7KiwzXGgtBbiZeCJAJSrpk5rqh0NFl4YYz4TkW5Y9ZXuDdwrjTGu4JWsRhYCTUr/0xCR\ng8A++9fmsqCWzH+ewvriYoxxDw4O4D5q0KB3Gsq97u4C9LpPAcotk2Z3yZwiIi8F4PzVUZ0pzRcA\n1wHfYo1FWGKCM4p4JVYtQ0ugABgBPF1hm2i3+5cCodgJZgFwh70swGAgzxhTrSoo0GBRKfs/m7Rg\nl6O2jDFTK8mr8VoDdYS3tSSPA9sJQLAI9utujMmsJC/V3+evDm/TvojIo8AqY8wC4BVgroikYzXc\neuqyGqiy/hP4P7usH7mXFVgCJAMfAU2Bc4DrA11OEXkLGA60FJFM4CGsK2qMMf/E6q11EZCO1aur\nRp0tdLqPBkREbsGqV3WvHnFi/bI7C/ixwi4tjDG9RWQbpzaCdQaGYTX0JwDuH6QWWL/wo7ycb4e9\n3+ehdE5UndPQP9PGmOsJEA0WSimlqqTjLJRSSlVJg4VSSqkq1ZsG7pYtW5qkpKRgF0Opemv16tUH\nqrNWc3XpdzY0VPd9rTfBIikpiVWrVgW7GErVWyJScYDqadHvbGio7vuq1VBKKaWqpMFCebXt4Da2\n5GxBe8wppepNNZTyHWMMf13xV97e9jYAF7S/gCfOfYJwZ3iQS6aUChYNFuoUn2V8xtvb3uaaHtfQ\nPKo5z617jr999zceOuuhYBct5BQVFZGZmUlhYWGwi+IzUVFRtG3blvDw4P44SLrv49PaP2PGWB+V\nRIEGC1WBq8TFs2ufpVtcN34/8Pc4HU6OFh3l35v+zYVJFzIkYUiwixhSMjMziYmJISkpieAsteBb\nxhhycnLIzMykY8eOwS6OCiHaZqHKWbFnBbuO7OLm3jfjdDgBuK3vbbSLacfj3z1OiakTs7MHTGFh\nIS1atKgXgQJARGjRokW9ulJSvqHBQpXz0Y6PiImIYUT7EWVpUWFR3NH3DtJz0/nipy+CWLrQVF8C\nRanS55Obm8vEiRPp0aMHycnJAI1FpLmILBKR7fZtnL3Paa3vrEKfBgtVxlXi4uusrxnedjiRzshy\neaM7jqZLbBdmr5+tvaMaiOnTpzN69Gi2bt3K999/D1CINc37YmNMV2Cx/RjKr+88DWt9Z1WPaLBQ\nZTbnbCbveB5DE4eekucQB1N6TmH7oe2s3LMyCKVTgXTkyBG++uorpk61ZluPiIgAa2Ep93Wx5wDj\n7funtb6zCn0aLFSZb3Z/gyCc1eYsj/kXdbqIuMg43tjyRoBLpgItMzOTVq1accMNN9CvXz9uuukm\nsP6/iHdbMGcPJ5eJ9ba+czkiMk1EVonIqv3791fMViFMe0OpMmv2rqF78+7ERcV5zI90RjKx20Re\n3vAyWflZJDbxtGZ9w/XIR5vYvPuwT4/Zs01THrqkl9f8hx9+mLS0NMLCrK9ycXExQ4YM8ZgGeEx/\n+OGHTzmuy+VizZo1PPvsswwePJjp06cDnOG+jTHGiEiN6iSNMbOB2QApKSlan1mHaLBQAJSYEjYe\n2MhFHS+qdLsrul3ByxteZsGS+7n14EGI7QDD7oIWnQNUUlXRvHnziI2NBaxG6ZkzZ3pM87atJ/Hx\n8bRt25bBg62FBSdOnMgzzzzTCGt97wRjTLZdzbTP3uW01ndWoU+DhQLgx7wfyS/Kp0+rPpVulxDV\ngsFEMn/fSn5d3ALH5rWwZQFc9xG06Rug0oamyq4A6ppWrVrRrl07tm3bRvfu3Vm8eDFYDdyl62LP\nsG/n27uc1vrOKvRpm4UCYP3+9QD0blXF6qJfPsSl+zLJCg9j9fin4bZvIbIpzLsGCnIDUFIVKM8+\n+yyTJ0+mT58+rFu3DqwlQWcAI0VkO3CB/Ris9Z13Yq3v/C/gtiAUWfmRXlkoANYfWE9MRAxJTZO8\nb7R7Haz4J+cPmELjwyuYnz6fgUP/ApNeh5cvgKV/hYv+HrAyK//q27dvuSnERcRljMkBzq+4rbH6\nU98ewOKpANMrCwXA1pyt9GzRE4dU8pFY9hhExdLo/EcY1WEUi35axHHXcUgcACk3wspX4JBPlzxQ\nSoUIDRYKYww783bSuVkljdR7N8EPn8GQ2yA6lguTLuRY8TG+3f2tlT/sbhAHpM4KTKGVUgGlwUKx\n99hejhUfo3NsJcFixT8hvBEMtAZpDUoYRNOIpiz6aZGV37QN9L0a1r0BBYcCUGqlVCBpm4ViZ+5O\nADo28zLL6IljsPED6HUZNGoOQLgjnBHtRrDk5yUUuYqstS4G3gRrXocN78KgmwNV/AatdevWTJky\nBYfD+t1XUlLC6NGjPaYBXtOVqooGC8WOvB0A3q8sti6EE0eg7zXlkkcljWL+jvmkZacxrO0wSDjT\n+ls9xwoc9WyCvVB02223cdttp3Y88pRWWbpSVdFqKMXOvJ3ERsbSPKq55w2+f8safNf+7HLJQxKG\n0Di8MYt/Xnwysd+1sHcD7NngxxIrpQJNg4ViZ+5OOjXr5DmzIBd+/MqqgnKU/7hEOCM4K+Esvsn6\n5uRMtL0uB3HC5g/9XGqlVCBpsGjgjDHsyNtBp1gvwWL7Iigphh4Xe8we1nYYe4/tZXvudiuhcQvo\nOAw2fQg6lblS9YYGiwbuYOFB8o7nee82u3UhNIm3xlJ4UDqd+deZX59M7DkODu6AvRt9XVylVJBo\nsGjgduZZPaE8VkMVFUL6l9D9olOqoEq1btSaHs178HWWW7BIvtQac7F5vsd9lO/s2rWLESNG0LNn\nT3r16sWsWTrORfmH9oZq4Eq7zXqshsr8Dk7kQ7cLKz3GsMRhvLrxVY6cOEJMRAw0bgntBluD+M77\nkz+KHZo+vc/3Dftn9IYxM7xmh4WF8dRTT9G/f3+OHDnCgAEDGDlyJFdccQUdO57sCn3gwAHmzZvH\n2LFjT0lPS0vzbZlVvaRXFg3cjrwdNA5vTHyj+FMzM1IBgfaeF0MqNaztMFzGdXI0N0DXUdZ/nId3\n+7bAqpyEhAT697eWu46JiSE5OZnU1FRuvvlmFi5cWPZXup6Ft3SlqqJXFg3czjyrJ5R4GhPxU6r1\nyzY6ttJj9G7Zm8bhjUnLTmNU0igrsduFsPgR2P4FDLje9wUPRZVcAQRCRkYGa9eu5c9//jNLliwJ\nallU/aNXFg3cztydnkduFx+HzJWQdOp63BWFOcIYGD+QFdkrTia27glN28IPX/iwtMqb/Px8JkyY\nwMyZM2natGmwi6PqIQ0WDdjhE4fZX7Df88jtrNVQXAgdzqnWsQYnDObnIz+zO9+udhKBbqNg5zIr\n8Ci/KSoqYsKECUyePJnLL7882MVR9ZQGiwasrHHbU0+ojFTrtsPZp+Z5MDjBWn6z3NVF1wuh6Chk\nfHNa5VTeGWOYOnUqycnJ3HXXXcEujqrHNFg0YD/m/QjgeYzFT99A615lEwdWpUtsF1pEtSAt261n\nTcdzISzKardQfpGamsrcuXNZsmQJffv2pW/fvnzyySfBLpaqh7SBuwHbkbuDCEcEbZq0KZ/hKoJd\n30G/X1X7WCLCoIRBfLfnO4wxVoN5RCNIGmaNAh/zuI9LrwCGDh16cqoVW0ZGBh9+qNOtKN/SYNGA\n7cyzGredDmf5jN1roehYtdsrSp2VcBaf/vgpy3dtYsW2CNJ25jAitxO3FixiyfJv+eWQITgdOhOt\nv0VERDB//nyWLVtWluZwOLymK1UdGiwasJ15O+nTss+pGaVtDDUMFv1bDwTgpnfeojDnbPq2i+Xn\n5mdD1kv87+O3+PvKYmZO6kv3M2JOt+iqEm3atGHp0qUe87ylK1UV/VnRQB0rOsbu/N2eR27/lAqt\nekCTVtU+3oH849wzbxclJ1oQ3/pnvrpnBB/cdg6P3Twe07wzt7Xdyf4jhVz63DfMX5flw2ei/MXl\nctGvXz8uvrhsEskIEVkhIuki8raIRACISKT9ON3OTwpWmZX/aLBooDIOZ2Awp/aEchXDz2k1uqpI\n35fPpc9+w/e7chl0xiCOh6WTEBtRli9dRxGfs5JPbxvImW1jmT5vHS/9b8cpde0qtMyaNYvk5GT3\npLbA08aYLsAhYKqdPhU4ZKc/DWgDVT2kwaKBKp1A8JQxFnu+t+aDSqpesNiYlceVL33LCVcJ7916\nNpN6n0cLysm3AAAgAElEQVR+UT6bcjad3KjrBVBcSKuclbw+dRBj+yTw2KdbeXD+JopdJb56SsqH\n9uzZw8cff8xNN90EUBrYY4B37U3mAOPt++Psx9j554vHKQFUXabBooHambsTpzhpH9O+fEbZ+Iqq\nR26v2JnD1bPTiA538t9bzuYXic0YfIY13qLcPFEdhkJYNGz/gqhwJ89e1Y9fn9uJuWk/cdPrqzhS\nWOSrp6V8ZMaMGTzxxBNlDeA5OTkALmNMsb1JJpBo308EdgHY+XlAC0/HFZFpIrJKRFbt37/fj89A\n+ZoGiwZqZ95O2jdtT7gzvHzGT6nQogvEeJhY0M3SrfuY8up3tG4aybu3nkXHlo0BiIuKI7l5cvlg\nER5ljbnY/gUYg8Mh3H9RMo9d3puvtx9g4ovf8sPeI75+ig1CYWEhgwYN4swzz6RXr1489NBDp33M\nhQsX0rx5cwYM8LyGyekwxsw2xqQYY1Jatap+m5gKPr/2hhKR0cAswAm8bIyZUSE/EngdGADkAJOM\nMRkiMhKYAUQAJ4B7jDE6M5oP7cjdcWoVVIkLfvoWeo33vJPt0w3Z/OattfRIiGHODYNo0SSyXP7Q\nxKHlpywH6DoStn8OOTugZRcArh7UnrZx0dw5bx0XP/sNv7+wOzec07HOdq99/LvH2Xpwq0+P2aN5\nD+4ddK/X/MjISJYsWUKTJk0oKipi6NChREdH89577xEffzLgO51Ofve733HPPfeckj5/fvl1R1JT\nU1m6dClJSUkUFhZy+PBhpk+fDuAUkTD76qEtUNpTIQtoB2SKSBjQDOv7rOoRv11ZiIgTeB4YA/QE\nrhaRnhU289YwdgC4xBjTG7gOmOuvcjZEJ1wn2HVk16mN23s3wvG8SicP/GzjHn7z1lr6tG3GmzcP\nOSVQAJzd5mxcxsV32d+dTOw60rpNX1Ru22FdW/H5787ll91a8ZePt3D5C6ls3n241s+toRERmjRp\nAlhzRBUVFSEi/PGPfyw3FXnpGhbe0t099thjLF26lIyMDObNm8d5553HG2+8AXAEmGhvdh1QGmUW\n2I+x85cY7b1Q7/jzymIQkG6M2QkgIvOwGsI2u20zDnjYvv8u8JyIiDFmrds2m4BoEYk0xuiMdD7w\nY96PuIyLrnFdy2eUtVd4btxetHkvd7y5ht5tmzHnxkHERIV73O7M1mfSOLwxqbtTOb/D+VZiXBK0\n7GZVRQ25tdz2LZtEMvvaASxcn80jH23i0ue+4eZzOzH9/K5EhTtPPUGIquwKwJ9cLhcDBgwgPT2d\n22+/ncGDB5Obm+uPU2UCd4nIX4C1wCt2+ivAXBFJBw4CV/nj5Cq4/NlmUdboZXNvEDtlm0oaxiYA\nazRQ+M6O3B2Ah55QP6Va/6k3q/g2waqMg9z+5hp6JVYeKADCHeEMOmMQy3cvL989tstIKyCdOHbK\nPiLCJWe24cu7fsnl/RN5cdkORs/8iuU7DtTqOTYkTqeTdevWkZmZyXfffcfGjb5b+3z48OEsXLiw\n9OEJY8wgY0wXY8wVpd9JY0yh/biLnb/TZwVQIaNaVxYi8mAVm+wzxvzTB+WpeN5eWFVTo7zkTwOm\nAbRv397TJsqD9Nx0wiSMjk3dqiBKSqxg0X3sKdtnHDjKza+vom1sNHNuGEjTSgJFqXPanMPSXUvJ\nOJxxcr2MriMh7XnI+NrrUq2xjSJ4YuKZjOubyB8+2MA1/1rBPRd25/YRXWr1XBuS2NhYRowYQWFh\nYVnao48+CsCKFSvIy8ujsLCQ9evXl+W731eqMtWthhqCdWnpreVxDlAxWJQ2epVybxCruM0pDWMi\n0hb4AJhijNnh6aTGmNnAbICUlBStI62m9Nz0U3tC7d8CBYdOGV9RcMLFza+vAuDV6wcS2yiC6jg7\n0ZraPDUr9WSw6HA2hDe2qqKqWNf7nC4t+Wz6udz3/nr+/vk2ThSX8LuR3ar5DBuO/fv3Ex4eTmxs\nLAUFBSxatIh7772X4mKrh2taWhrz5s0jOzubsWPHkpeX5z4imzlz5ng7tFLlVDdYuIwxXlsdRcTT\nf9Qrga4i0hErKFwFXFNhm9KGsW9xaxgTkVjgY+A+Y0xqNcuoqik9N53k5snlE720Vzy6cDPb9+Uz\nd+ogkuzusdXRLqYdnZp1Yumupfyqpz17bVgkdPplWRdaqhi3FR3h5B9X9iXc6WDW4u0kxkVzZUq7\nSvdpaLKzs7nuuutwuVyUlJRw5ZVX0qRJk7I2C6fTSdOmTYmMjKRJkyYUFxfTrFmzsv117Jyqruq2\nWVT1q/2UfLsN4g7gc2AL8I4xZpOIPCoil9qbvQK0sBvG7gLus9PvALoAD4rIOvuvdTXLqipRUFxA\n5pFMusRWqNb56Rto1g7iOpQlLd6yl7e++5lbh3dmWNea94m/oMMFrNq7ioOFB08mdh0JuT/Dvi3V\nOobTITx2eW+GdW3JH97fwLpdfmm4rbP69OnD2rVrWb9+PRs3buTBB8vXGGswUL5S3WARLiJNvfw1\nwxpHcQpjzCfGmG7GmM7GmL/aaQ8aYxbY9z02jBlj/mKMaWyM6ev2t88XT7ih25m3E4OhS5xbsDAG\nflpe7qri2IliHpy/ie7xMdxVy+qfkR1GUmJKWPqz20ynPS4GccDm6q+3EO508Nw1/YlvGsVv31qr\nI76r0LhxY55//nnGjx/PihUrGDt2LIcPH8YYw6xZsxg7dmzZX2l1lVJVqW41VBpwp5c8AT71TXGU\nv6UfSgcq9IQ68AMc3V+uvWLWl9vJyi3g3VvOItxZu05z3eO6k9gkkUU/L2JCtwlWYpPWVlDa9CEM\nv7/KqqhSzaLDmXVVX6586Vsemr+Jf0zqW6sy+UPZYk8hYuDAgSxaZI1neeSRR8rK9vXXXzNixIiy\n7YwxXHLJJafsr0MklCfVDRaDqXkDtwpBWw9uJTosuvycUBXWr9h18Bivpv7IFQPakpJUvWVVPRER\nRnUYxdzNc8kpyKFFtN0rutd4+PhuqyoqvuI4Te9Skppzx3ldeWbxdi45sw0jegS/ZjIqKoqcnBxa\ntGgRUgGj1IoVK5g3b57XAHDddddxyy23lD02xpCTk0NUVFSgiqjqCH82cKsQtClnE93juhPmcHvr\nM76BmARobo3onrV4OyLCXaNOv/fRuC7j+Pemf7Nw50Ku62UP8k2+FD65x6qKqkGwALh9RGc+2ZDN\nnz7cyBe/O5fGkcFdv6tt27ZkZmYSqpPiFRQUkJXlff2Q/Px8tmwp334UFRVF27Zt/V00VcdU95tW\n4wZuFXpcJS62HtzKZV0uO5lojDW+ImkYiJC+7wjvr8nkxnM6ktAs+rTP2Tm2M31a9eH97e8zpecU\n69d3aVXUhndrVBUFEBnm5PEJvZn4z2958ottPHRJr9Mu4+kIDw/3OGVGqIiJiam4JkU5TZs2rTRf\nqVJ+beBWoSXjcAYFxQX0aun2H+yB7ZC/FzoOA+CFZTuIDHNy6/DOXo5Sc5d3uZydeTtZs2/NycQz\nr4aDO+Dnb73v6MWADs351eAOvLY8g++1d1SlioqKOHz4sMe/vLw8XC5XsIuo6oiaNnB7+wn4mW+K\no/ypdEGins3dqn5+/J912/Fc9uQV8tH3u5k8uIPHCQJra0zHMcxcM5PXNr7GgHh72ute4+HTe2HN\nXGuwXg39fnR3vti8hz98sIH5t59DWC0b4eu7IUOGMHPmTK9tFqNHjw5wiVRdVa1gYYx5xN8FUf63\nfv96osOiT46oBmvqjaZtIa4jr322DVeJYepQ31arNApvxDXJ1/DCuhfYfmi7NYFhRGPoPQHWvwNj\nZkBUs6oP5CYmKpyHLunFbW+s4bXlGdw0zMNa4son61soBbr4UYOyeu9q+rXuh9Nh1xqWlFiN2x2H\nkX/CxRsrfmJM7wTaNW/k83Nf3f1qosOiefH7F08m9psCRcesgFELY35xBuf1aM0/Fv1AVm6Bj0qq\nlPJEg0UDkVuYS3puOinxKScT922GYzmQNIwF63ZzpLCYG8/xT2NtbFQsN/ziBhb9tIg1e+22i8T+\nkJgC3z5vLbxUQyLCI5f2whh4eMGmqndQStWaBosGYvW+1QAn2wzAqoIC6DiMt1ftolt8E/q3j/Vb\nGa7reR2tG7XmiZVP4CpxWb2gzv4NHPoRti6s+gAetGveiDsv6MqizXv5fNMeH5dYKVVKg0UDkZqV\nSnRYNL9o+YuTiT9+DXFJbCuM4/tduUwa2N6vA8sahTfidwN+x6acTczbNs9KTL4E4jrC1/+wqsVq\n4cahHelxRgwPL9hE/nGdvkIpf9Bg0QAYY/jfrv8xNHEoEU57ivHiE1ZPqE7DeXvlLsKdwmX9Tl30\nyNfGdhzLOYnnMGvNLLLys8DhhF/eC9nrYPMHtTpmuNPB3y7vzZ7Dhfzjix98XGKlFGiwaBA252xm\nX8E+hrcbfjLx52/hRD5FnUfywdpMRvU8g+aNq7dWxekQER4c8iCC8Oi3j1pdOvtcCfG/gC8fgeLa\nLYjYv30ckwe357XlP7IxK8/HpVZKabBoABbuXEi4I5xzE889mbj9C3BGsLiwB4eOFXHlwMCtE9Gm\nSRvuHHAny3cvZ8GOBdbVxag/Q+5P8NXfa33cey7sQYsmkfzhgw24SnRSAaV8SYNFPVdQXMBHOz/i\n/PbnExvl1ni9/QvocA5vrjtIm2ZRDO3SMqDlmtR9Ev1a9+PJVU9yqPAQdD4PzrzGarvYvbZWx2wW\nHc6DF/dkfWYec7/N8Gl5lWroNFjUc+9se4e843lc1eOqk4kHd8KBH8htO4Kvt+9nYko7nI7Azpjq\nEAcPDHmA/BP5/GP1P6zE0X+z5o165zo4drDyA3hxcZ8EftmtFU9+8QN78gqr3kF5tGvXLkaMGEHP\nnj3p1asXs2bNAkBEmovIIhHZbt/G2ekiIs+ISLqIrBeR/kF9AsrnNFjUY9n52bz0/UuclXBW+S6z\nm6yFh94/1g+AKwYEZ4bRrnFdmdJrCh+mf8iqPasgOg6unAtHsuGdKVBU8//sRYQ/j/sFRa4SHvlI\nx17UVlhYGE899RSbN28mLS2N559/HiAKazXLxcaYrsBiTq5uOQboav9NA170cFhVh2mwqAeMMeQW\n5nKg4ADFJcUYY9h0YBPTFk3DZVw8MOSB8jtseh+TmMIrG4s5p3NLv4zYrq5bzryFxCaJ/DntzxSV\nFEG7gXDpc9bI8nnX1CpgtG/RiOkXdOXTjXv4Qsde1EpCQgL9+1sXB24z10YA47DWr8G+HW/fHwe8\nbixpQKyIJAS21MqfgrsYgDotxhjm75jP7PWz2XVkFwCC4BAHLuMiNjKWFy54gXZN3RqvD6TDng3s\n7P8HsnYUcO+YHkEqvSU6LJp7B97Lb5f+lvd+eM+qLjtzErhOwILfwJxLYNJ/ICa+Rse9eVgnFqzb\nzQPzNzKkcwuaRoX76RnUfxkZGaxduxYgH+hkjMm2s/YApW9MIrDLbbdMOy0bVS9osKijjDE8teop\n5myeQ5+WfZjUfRIRzggOFR6iuKSYhCYJjOowimaRFSbo+/4tEAf/PtSXZtHCqJ41+0/YH4a3G87A\nMwbywroXGNtpLDERMdD/WoiMgQ9vhX+NgElzIXFA1QezhTsdPD6hD5e9kMrjn27lr5f19uMzqL/y\n8/OZMGECM2fOZMKECeVGTRpjTE0XPhORaVjVVLRv376KrVUo0WBRR32Y/iFzNs/hqu5Xcf/g+3FI\nNWoUi0/Amtc50Wkk72wt4ZrB7YkKD/5SJCLC3Sl3c9XCq3h146tM7z/dyug13lq9b95keHU0jHkC\nBlxf7cWSzmwXy43ndOTlb35kXN9EBnWs/RKxDVFRURETJkxg8uTJXH755aXJe0UkwRiTbVcz7bPT\nswD3/tdt7bRyjDGzgdkAKSkp2r+5DtE2izpo79G9PLHyCQaeMbD6gQJgywI4uo+lMRdzwlXCpACO\nrahKrxa9uLjTxczdPJfsfLeai4Q+8Ov/WSv5LbwT5t8ORdWfYfauUd1oGxfNfe+vp7BIF/qpLmMM\nU6dOJTk5mbvuuss9awFgr4/LdcB8t/Qpdq+oIUCeW3WVqgc0WNRBL61/iUJXIY+c9Uj1A0WJC776\nO6Zld/6xoy1ntoslOaGpfwtaQ7/t91uMMTy37rnyGY2aw+T/WtOCrHsDXhkJB3+s1jEbRYTxt8t6\ns3P/UZ5fmu6HUtdPqampzJ07lyVLltC3b1/69u0L0AyYAYwUke3ABfZjgE+AnUA68C/gtiAUW/mR\nVkPVMbuO7OKD7R8wsdvE8g3XVVn/Nuzfyo/Dn2XbZwXMuLyL/wpZSwlNEpjcczKvbXyNKT2n0L15\n95OZDieM+AO06Q8fTIPZv4RJb5QtB1uZc7u14vL+iby4bAdj+yTQ44zQCpKhaOjQoaesriciecaY\nHOD8itsba+PbA1S8gEm67+PT2j9jxlgflST49MqijvnP5v8gIkzrMw2MgbxM2LcVThz1vtPh3fD5\nH6DtQF7c15vGEU4uObNN4ApdAzf1vommkU15evXTnjfoPhqm/Q+anAFvTIT0xdU67gNje9IsOpx7\n39OpQJSqDQ0WdcjRoqMs2LGAC5MupNVPafDCEHi6F7wwGGa0h1cuhNRnIGfHyZ1yd8F/JoCriCOj\nn2Hhhr1c2jeRxpGheVHZNKIp03pPI3V3Kst3L/e8UfOOcMMn0KIrvHUVbF9U5XHjGkfw0KW9+H5X\nLq8tz/BtoZVqADRY1CELdywkvyifq3Pz4O1fAWL1EJrwCpz9W2uJ0kUPwLP94fkh8OYkeH4Q5P4M\nV73BWzsjKShyMXlwaHdZvKrHVSQ2SeTp1U9TYryscdG4JVz/EbROhrevhV3fVXncS/okcF6P1jz5\n+TZ2HTzm41IrVb9psKgjjDHM2zaPnuGx9F79BgyaBrd8DYN/Db0nwgUPWY/v3ACjH4cmrawg0Xsi\n3PI1RR3O5bXUDM7q1IJfJDar+oRBFOGM4Df9fsPWg1v5eGcldcbRcTD5PWiaAG9cAfu2VHpcEeHP\n43+BQ+CPH248pU5eKeWdBos6YtXeVaTnpnP17h1IylTrisLpYVRybHsYcgtc9xHc9i1c+iw078Qn\nG7LZnVfITcP8s8a2r43pOIbk5sk8u/ZZjrsqWeOiSSu49gMIi4K5l1vVbpVIjI3m96N78NUP+/lg\n7SnDAJRSXmiwqCPe2vhvmpUYRjfrbgWKGix/WlJieHHZDjq1asyI7q39WErfcYiDu1PuJvtoNm9t\neavyjeOS4Nr3rUb+uZfB0ZxKN//VkA70bx/Lows3cyC/dostKdXQaLCoA/Yd3cuSrK+5LP8YUeNf\nAmfNGqc/3pDN1j1HmH5+VxwBnor8dAxOGMzQxKHM3jCbg4VVTFke3wuumQd5u+DNK+B4vtdNnQ5h\nxoQ+HD1ezKMfbfZxqZWqnzRY1AHvfvUgJcZw5ZnToFW3Gu1b5Crh6S9/oFt8Ey7uE5rdZSvzfyn/\nR0FxAX9fWY0V9DqcDRP/bS2e9M4Ua3oTL7rFx3DHiK4s+H43b6/82YclVqp+0mAR4opy0nk3+xvO\nIZp2Q39f4/1f/eZHdu4/yu8v7BHwBY58oXNsZ6b+YioLdy4kNSu16h16XASXzIIdi2H+bVDipTcV\ncMd5XRjapSUPfLiJdbtyfVhqpeofDRahrKSEpQtuZr/TwVVD7gVHzd6un3OOMfPL7VyQHM8FITC7\nbG1N6zONjs068si3j5B3PK/qHfpPgfMfhA3/taY5dxV73MzpEJ69uh+tYiK5Ze5qsnKrP+eUUg2N\nBosQZr59njknskgMb8bQbpfVaN/CIhe3vbmacKfwyLhefiphYEQ4I/jLOX9h/7H9PJD6QPW6vA69\nC355H6z7D7x7PRR7bsiOaxzBv6akcPREMde+vEIbvJXyQoNFqNqzge9SZ7A+KpIb+t+B01H9qcSL\nXSXc/c73bMw6zFNX9iUxNtqPBQ2MPq368LsBv2PprqW8svGVqncQgRH3w+gZsOUjeG0s5HnuKtuz\nTVP+ff1AducVcM2/0titVxhKnUKDRSg6cQzz3k28FBdLq6gWjO9a/auK/OPF3P7mGj7ekM0fL0pm\nZB2ufqro2p7XMjppNLPWzOKD7R9Ub6cht8KVr1sD9l461wocHqQkNefV6weSnVvI5S8sZ2NWNaq7\nlGpAQnOCoIbMGJh/O0uO/szKJi25v880Ip2RVe5W7Crhs017mPHpVrLzCnnw4p7cOLRuDMCrLhHh\nr0P/yuETh3lo+UMcPnGYKT2nIFWNOek5Dlolw7s3WtOkdB8LIx+Bll3LbXZ255a8c8tZ3PjaSi5/\nYTm/H92dG8/pWKe6G6v6J1RmvtUri1BiDCx+hCObP+CJxCS6xHbhyu5Xet38QP5xPl6fzQMfbuTs\nGUu44821NI4I462bh9S7QFEqwhnBzBEzuaDDBTy56kluX3w7GXkZVe/YqhtMWwoXPAI7l1pzZr13\nM2Sttl53W3JCUz757TCGd2/FXz7ewqXPf8O3Oyof5KdUQ6BXFqGixAWLHqTk2+d4qNsA9hYf5LWz\nHiLMcfItyisoYsXOHJbvyGH5jgP8sNcaeNY4wslZnVsyaWA7RnRvRZizfv8GiA6L5slfPskbW97g\n2bXPcsmHlzDwjIEMPGMg7WLa0TK6JTHhMTQOb0yTiCY0Dm9MlDMKcYbD0Duh72RY/gysfBk2vAPx\nvaHfryD5EmiWSFzjCF66dgALvt/N459u5ep/pXFmu1iuO6sDI3vGExPlYZoVpeo5vwYLERkNzAKc\nwMvGmBkV8iOB14EBQA4wyRiTYefdD0wFXMBvjTGf+7OsQXUoAxb8lqIf/8ffks9mUWEmdw+4m26x\nv+CrH/aTuuMA3+7IYWNWHiUGosIdDExqzvh+iZzVqQW9E5vV+wBRkUMcXNvzWsZ0HMN/t/2XxT8v\n5sV1L2Lw3FPKKU4ahzcmLiqOMxqfQULjBBIuvIeEg7s4I+NbEr78I60+v49GCX2R5EuQTiMY1+dM\nRvU8g/+u3sVrqRnc9c73RDgdnN2lBYM7tmBAhzj6tG0WEuuYK+VvfgsWIuIEngdGApnAShFZYIxx\nn19hKnDIGNNFRK4CHgcmiUhP4CqgF9AG+FJEuhlj6s8iysUnIHMlrJ+Ha91bpEVH83SPgWwrzKRn\no/HM/6oLf33rC4pchjCH0K99LL85rytnd25B3/axRIbpf1AALaNbcmvfW7m1760cLTrK3mN7ySnI\n4WjRUfKL8jl6wr4tOsrRoqPkFOaQfTSb5VnL2V+w3woukUA7a3R7uMkh7oeXid0ymzgcxEU1p2VM\nW65P6cLxsC5s2d+SNTv3sWzbPkBwCLRv3ogurZvQpXUMnVs1JjEumsTYaM5oFqXvk6o3/HllMQhI\nN8bsBBCRecA4wD1YjAMetu+/CzwnVmvlOGCeMeY48KOIpNvH+/Z0CpSafoDNuw9jMPyQ/xX5xQcx\nQMvDmwkrOWZXXRvAWP9K67KNsX+x2reGssfgdt+U7VmWWlofbjA4TRFhJYVEuI4RXZzLUQdkhkWw\noV0iBY4SSvILOL73GtYcO5PeicKNQztydueWDEyKo1GE1hhWpXF4Yzo160SnZp2qtX2Rq4g9x/aw\n5+gedufv5mDhQQ4dP0TukWwO5f7IofxsNp84zP7cjRQcdvvYtoRWLYRWRBDniqCJKxJXfhgbDwob\ntwiOEieCYBDCw5xEOB2E2X/hTgdOh+DAarAXAYd9K5RvSK+sWV2AQ407URjR3GO+Qxz0bXZJtV6H\nplHhXDmwBkv0qgbJn/8DJQLu80VnAoO9bWOMKRaRPKCFnZ5WYd/EiicQkWnANID27ate0OezjXuY\nm/YTAI06vIuzURVzAkmF28o2NeaUzaX0r1zNiJVraIrDROCkNU0c7enbJIWhbYaTfEEcvRO1aiMQ\nwp3htItpR7uYyv+jNMcOkZu1kuzd35F9KJ3dx/aSffwQ2a5jZFPA1jAoCIeixkJxDWYDLn+SWuyT\nv9774UrCWPhN9dZZb9+8kQYLVaU6/XPVGDMbmA2QkpJS5dftj2OT+f3o7ogIBUXDMJQgIjiKjuHA\nAILD4UDEiYgDh8OBAwficFjbORwIDhwOJ4gg4qi626aq86RRHHFdRxHXdRQ9K9uwpIQS46LYdYLi\nkmIwJSf/SsrXoJqyq1JwlRiKSkooKTcy3c738KkuSwqLBrsDhKdR7U0iYqoVgxz6GVbV4M9gkQW4\n/1xpa6d52iZTRMKAZlgN3dXZt8aiwp1lv9ibRDZ1ywntleNUHWH/uIhwhhMR7LIEQVUdWlTd5s8u\nNCuBriLSUUQisBqsF1TYZgFwnX1/IrDEWD+RFgBXiUikiHQEugJVL7KslAoKtw4tY4CewNV2RxVV\nT/jtysJug7gD+Bzrl8arxphNIvIosMoYswB4BZhrN2AfxAoo2Nu9g9UYXgzcXq96QilV/1SnQ4uq\nw6S+LFovIvuBnwJ0upbAgQCdS88dGufXc0MHY0wrTxuJyERgtDHmJvvxtcBgY8wdFbYr65QCdAe2\n+ahswVIfyuD1fXVXpxu43VXnyfqKiKwyxqQE6nx67uCfX8/tG+6dUk5XsD+PDa0MDWvYr1LKX/zS\nKUWFDg0WSilfqE6HFlWH1ZtqqADzyWW0nrtOnV/PXQlvHVr8WrLgfx6hAZWh3jRwK6WU8h+thlJK\nKVUlDRZKKaWqpMGiBkTk7yKyVUTWi8gHIhJrpyeJSIGIrLP//umn848WkW0iki4i9/njHG7naici\nS0Vks4hsEpHpdvrDIpLl9lwv8tP5M0Rkg32OVXZacxFZJCLb7ds4P5y3u9tzWycih0XkTn89bxF5\nVUT2ichGtzSPz1Msz9jv/3oR6e+Hcwf1M66qJiKNRKSP/Vf1msu+YozRv2r+AaOAMPv+48Dj9v0k\nYKOfz+0EdgCdgAjge6CnH8+XAPS378cAP2BN4/Aw8H8BeK0zgJYV0p4A7rPv31f6+vv5Nd8DdPDX\n83ST+RQAAATOSURBVAbOBfq7f368PU/gIuBTrKmLhwAr/HDuoH3GqyjrQOAMt8dTgPnAM0DzAJy/\nC3COh/RhQOcAvQbhwEys2S5WA2uwuieXflb6+vP8emVRA8aYL4wxxfbDNKy+5IFSNp2CMeYEUDqd\ngl8YY7KNMWvs+0eALXiYJj7AxgFz7PtzgPF+Pt/5wA5jjN9mBjDGfIX15Xfn7XmOA143ljQgVkQS\nfHnuIH/GK/MScAJARM4FZmCtsplHYHoDzQSOeEgvsPMC4SmgCdaI6wHGmP5AMtBJRF4EPvDnyTVY\n1N6NWL/ySnUUkbUi8j8RGeaH83laHyQg/3mLSBLQD1hhJ91hV1O86o+qIJsBvhCR1fYUEQDxxphs\n+/4eIN5P5y51FfCW2+NAPG/w/jwD/RkI9Ge8Mk5jTGlgmwTMNsa8Z4x5AOtXv78lGWNOWUDEGLMK\n66orEC4CbrZ/vJWe/zBwK9Zn9Wp/nlyDRQUi8qWIbPTwN85tmz9iTXD4hp2UDbQ3xvQD7gLeFPn/\n9u4mRKsqDOD4/5kUE6ONtHDTYoo2tXDRUshMREUCCcI+iAqjQIJZBblRAsGNrgQX7pQ0irSvRQQK\n0SboY5MfqyKCiJlCkkCIyKfFOe/MdfSdg/TeOyP8f5v3fc+dmecc5nKf+z733nPi/lv/+t0nIu4D\nPgRm6o55HHgI2EgZ95GeQm+qZ047gH31bHJelu/dvd33XR8sexr4oDYNNe6b9D3OcVbgPn5PlGUM\noHzju9DZNsTzYvcusW3tAPEBbtT94SZZJln9vX7b7I0P5S2SmVuX2h4RLwO7gKdG/7gsy7/+Xd9/\nFxE/Ao8A306wa4NPpxARqymJ4t3MPAuQmbOd7SeAz/qInZm/1te5iDhHKcPNRsSGzPytll/m+ohd\n7QC+H413qHFX48Y5yD6wjPv4Us4AX0bEH5TSz1e1rw9TSlF9+yYiXsvME93GiNhLuX4whMsR8VJm\nnlzUhxcpZeJemSzuQJTFXd4CnsjM6532B4CrmflvRExT1t/4acLh56dToBwg9gDPTzjGvIgIyhTy\nVzLzaKd9Q6dEshu4eLvf/5+x1wFTmflXfb8NeIeF9U8O19ePJx274zk6Jaghxt0xbpyfUEph71GW\nKL7W6dNELPM+PlZmHoqI85QbL77onGFPAW8O0IUZ4FxEvMBCcniccrPJ7gHiA+wDzkbEq4v6sHaI\nPvgE9x2Isu7GGspqfgBfZ+YbEfEM5WD2D3ADOJCZn/YQfyflYtpoOoVDk47RibWJcvb2A2VMAPsp\nB9GNlNLIz8DrPRywplm4WLcKOF0PFuuB94EHKdPRP9upY08y/jrgF2A6M6/VtlP0MO6IOANspkwz\nPQscAD7iNuOsCfwYsB24DrxSa+aTjP02y7iPr3QR8STwWP14KTMvLPXzPfVhC/Bo/Xg5M88PEtdk\nIUlq8QK3JKnJZCFJajJZSJKaTBaSpCaThSSpyWQhSWryoTz1LiIOUmZJHU1Qt4oySd0tbZl5cOj+\nSWozWWgoezLzT4C6RsLMmDZJK5BlKElSk8lCktRkspAkNZksJElNJgtJUpPJQpLU5K2zGsIccDIi\nRutiTAGfj2mTtAK5noUkqckylCSpyWQhSWoyWUiSmkwWkqQmk4Ukqek/CuUDW3RRMXYAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2109be8a470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "plt.subplot2grid((2,3),(0,0))             # 在一张大图里分列几个小图\n",
    "data_train.Survived.value_counts().plot(kind='bar')# plots a bar graph of those who surived vs those who did not. \n",
    "plt.title(u\"获救情况 (1为获救)\") # puts a title on our graph\n",
    "plt.ylabel(u\"人数\")  \n",
    "\n",
    "plt.subplot2grid((2,3),(0,1))\n",
    "data_train.Pclass.value_counts().plot(kind=\"bar\")\n",
    "plt.ylabel(u\"人数\")\n",
    "plt.title(u\"乘客等级分布\")\n",
    "\n",
    "plt.subplot2grid((2,3),(0,2))\n",
    "plt.scatter(data_train.Survived, data_train.Age)\n",
    "plt.ylabel(u\"年龄\")                         # sets the y axis lable\n",
    "plt.grid(b=True, which='major', axis='y') # formats the grid line style of our graphs\n",
    "plt.title(u\"按年龄看获救分布 (1为获救)\")\n",
    "\n",
    "\n",
    "plt.subplot2grid((2,3),(1,0), colspan=2)\n",
    "data_train.Age[data_train.Pclass == 1].plot(kind='kde')   # plots a kernel desnsity estimate of the subset of the 1st class passanges's age\n",
    "data_train.Age[data_train.Pclass == 2].plot(kind='kde')\n",
    "data_train.Age[data_train.Pclass == 3].plot(kind='kde')\n",
    "plt.xlabel(u\"年龄\")# plots an axis lable\n",
    "plt.ylabel(u\"密度\") \n",
    "plt.title(u\"各等级的乘客年龄分布\")\n",
    "plt.legend((u'头等舱', u'2等舱',u'3等舱'),loc='best') # sets our legend for our graph.\n",
    "\n",
    "\n",
    "plt.subplot2grid((2,3),(1,2))\n",
    "data_train.Embarked.value_counts().plot(kind='bar')\n",
    "plt.title(u\"各登船口岸上船人数\")\n",
    "plt.ylabel(u\"人数\")  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是得到了像下面这样一张图：<br>\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>bingo，图还是比数字好看多了。所以我们在图上可以看出来:<font><br>\n",
    "* <font color=red>被救的人300多点，不到半数；<font><br>\n",
    "* <font color=red>3等舱乘客灰常多；遇难和获救的人年龄似乎跨度都很广；<font><br>\n",
    "* <font color=red>3个不同的舱年龄总体趋势似乎也一致，2/3等舱乘客20岁多点的人最多，1等舱40岁左右的最多(→_→似乎符合财富和年龄的分配哈，咳咳，别理我，我瞎扯的)；<font><br>\n",
    "* <font color=red>登船港口人数按照S、C、Q递减，而且S远多于另外俩港口。<font><br><br>\n",
    "\n",
    "<font color=red>这个时候我们可能会有一些想法了：<font><br><br>\n",
    "\n",
    "1. <font color=red>不同舱位/乘客等级可能和财富/地位有关系，最后获救概率可能会不一样<font><br>\n",
    "2. <font color=red>年龄对获救概率也一定是有影响的，毕竟前面说了，副船长还说『小孩和女士先走』呢<font><br>\n",
    "3. <font color=red>和登船港口是不是有关系呢？也许登船港口不同，人的出身地位不同？<font><br>\n",
    "\n",
    "<font color=red>口说无凭，空想无益。老老实实再来统计统计，看看这些属性值的统计分布吧。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2109be8a278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAETCAYAAAAs4pGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngJAkNRkQkqSm/wWHQE5m/i4NfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x210ae12dfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看看各乘客等级的获救情况\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_0 = data_train.Pclass[data_train.Survived == 0].value_counts()\n",
    "Survived_1 = data_train.Pclass[data_train.Survived == 1].value_counts()\n",
    "df=pd.DataFrame({u'获救':Survived_1, u'未获救':Survived_0})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"各乘客等级的获救情况\")\n",
    "plt.xlabel(u\"乘客等级\") \n",
    "plt.ylabel(u\"人数\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>得到这个图：<font><br>\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/9.png?imageView/2/w/450/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>啧啧，果然，钱和地位对舱位有影响，进而对获救的可能性也有影响啊←_← <font><br>\n",
    "<font color=red>咳咳，跑题了，我想说的是，明显等级为1的乘客，获救的概率高很多。恩，这个一定是影响最后获救结果的一个特征。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x210ae18f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEUCAYAAAAx56EeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x210ae18bd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看看各登录港口的获救情况\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_0 = data_train.Embarked[data_train.Survived == 0].value_counts()\n",
    "Survived_1 = data_train.Embarked[data_train.Survived == 1].value_counts()\n",
    "df=pd.DataFrame({u'获救':Survived_1, u'未获救':Survived_0})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"各登录港口乘客的获救情况\")\n",
    "plt.xlabel(u\"登录港口\") \n",
    "plt.ylabel(u\"人数\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/Embarked.png?imageView/2/w/500/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>并没有看出什么...<font><br>\n",
    "\n",
    "<font color=red>那个，看看性别好了<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x210ae25d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAETCAYAAAAs4pGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoQkqWRASJJK/w84ssOgpyz/DwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x210ae20a4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看看各性别的获救情况\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_m = data_train.Survived[data_train.Sex == 'male'].value_counts()\n",
    "Survived_f = data_train.Survived[data_train.Sex == 'female'].value_counts()\n",
    "df=pd.DataFrame({u'男性':Survived_m, u'女性':Survived_f})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"按性别看获救情况\")\n",
    "plt.xlabel(u\"性别\") \n",
    "plt.ylabel(u\"人数\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/10.png?imageView/2/w/450/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>歪果盆友果然很尊重lady，lady first践行得不错。性别无疑也要作为重要特征加入最后的模型之中。<font><br>\n",
    "\n",
    "<font color=red>再来个详细版的好了<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAYAAAC3Y/QeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x210ae2daf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#然后我们再来看看各种舱级别情况下各性别的获救情况\n",
    "fig=plt.figure()\n",
    "fig.set(alpha=0.65) # 设置图像透明度，无所谓\n",
    "plt.title(u\"根据舱等级和性别的获救情况\")\n",
    "\n",
    "ax1=fig.add_subplot(141)\n",
    "data_train.Survived[data_train.Sex == 'female'][data_train.Pclass != 3].value_counts().plot(kind='bar', label=\"female highclass\", color='#FA2479')\n",
    "ax1.set_xticklabels([u\"获救\", u\"未获救\"], rotation=0)\n",
    "ax1.legend([u\"女性/高级舱\"], loc='best')\n",
    "\n",
    "ax2=fig.add_subplot(142, sharey=ax1)\n",
    "data_train.Survived[data_train.Sex == 'female'][data_train.Pclass == 3].value_counts().plot(kind='bar', label='female, low class', color='pink')\n",
    "ax2.set_xticklabels([u\"未获救\", u\"获救\"], rotation=0)\n",
    "plt.legend([u\"女性/低级舱\"], loc='best')\n",
    "\n",
    "ax3=fig.add_subplot(143, sharey=ax1)\n",
    "data_train.Survived[data_train.Sex == 'male'][data_train.Pclass != 3].value_counts().plot(kind='bar', label='male, high class',color='lightblue')\n",
    "ax3.set_xticklabels([u\"未获救\", u\"获救\"], rotation=0)\n",
    "plt.legend([u\"男性/高级舱\"], loc='best')\n",
    "\n",
    "ax4=fig.add_subplot(144, sharey=ax1)\n",
    "data_train.Survived[data_train.Sex == 'male'][data_train.Pclass == 3].value_counts().plot(kind='bar', label='male low class', color='steelblue')\n",
    "ax4.set_xticklabels([u\"未获救\", u\"获救\"], rotation=0)\n",
    "plt.legend([u\"男性/低级舱\"], loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/11.png?imageView/2/w/700/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>那堂兄弟和父母呢？<font>\n",
    "<font color=red>大家族会有优势么？<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PassengerId\n",
       "SibSp Survived             \n",
       "0     0                 398\n",
       "      1                 210\n",
       "1     0                  97\n",
       "      1                 112\n",
       "2     0                  15\n",
       "      1                  13\n",
       "3     0                  12\n",
       "      1                   4\n",
       "4     0                  15\n",
       "      1                   3\n",
       "5     0                   5\n",
       "8     0                   7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = data_train.groupby(['SibSp','Survived'])\n",
    "df = pd.DataFrame(g.count()['PassengerId'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PassengerId\n",
       "Parch Survived             \n",
       "0     0                 445\n",
       "      1                 233\n",
       "1     0                  53\n",
       "      1                  65\n",
       "2     0                  40\n",
       "      1                  40\n",
       "3     0                   2\n",
       "      1                   3\n",
       "4     0                   4\n",
       "5     0                   4\n",
       "      1                   1\n",
       "6     0                   1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = data_train.groupby(['Parch','Survived'])\n",
    "df = pd.DataFrame(g.count()['PassengerId'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>好吧，没看出特别特别明显的规律(为自己的智商感到捉急…)，先作为备选特征，放一放。<font><br>\n",
    "<font color=red>看看船票好了<font><br>\n",
    "<font color=red>ticket是船票编号，应该是unique的，和最后的结果没有太大的关系，不纳入考虑的特征范畴<font><br>\n",
    "<font color=red>cabin只有204个乘客有值，我们先看看它的一个分布<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G6                 4\n",
       "B96 B98            4\n",
       "C23 C25 C27        4\n",
       "F2                 3\n",
       "E101               3\n",
       "D                  3\n",
       "F33                3\n",
       "C22 C26            3\n",
       "C52                2\n",
       "B28                2\n",
       "C83                2\n",
       "F G73              2\n",
       "C126               2\n",
       "D26                2\n",
       "E44                2\n",
       "D17                2\n",
       "C2                 2\n",
       "B20                2\n",
       "C68                2\n",
       "D33                2\n",
       "B49                2\n",
       "F4                 2\n",
       "E24                2\n",
       "B51 B53 B55        2\n",
       "C123               2\n",
       "B35                2\n",
       "C93                2\n",
       "E33                2\n",
       "B18                2\n",
       "B57 B59 B63 B66    2\n",
       "                  ..\n",
       "C32                1\n",
       "C110               1\n",
       "C99                1\n",
       "D45                1\n",
       "C82                1\n",
       "E31                1\n",
       "F38                1\n",
       "E68                1\n",
       "E38                1\n",
       "B82 B84            1\n",
       "A6                 1\n",
       "A31                1\n",
       "T                  1\n",
       "F E69              1\n",
       "B19                1\n",
       "A14                1\n",
       "C128               1\n",
       "B78                1\n",
       "C85                1\n",
       "D46                1\n",
       "C118               1\n",
       "D48                1\n",
       "C103               1\n",
       "D49                1\n",
       "B73                1\n",
       "C46                1\n",
       "C62 C64            1\n",
       "C45                1\n",
       "C30                1\n",
       "D21                1\n",
       "Name: Cabin, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ticket是船票编号，应该是unique的，和最后的结果没有太大的关系，不纳入考虑的特征范畴\n",
    "#cabin只有204个乘客有值，我们先看看它的一个分布\n",
    "data_train.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>这三三两两的…如此不集中…我们猜一下，也许，前面的ABCDE是指的甲板位置、然后编号是房间号？…好吧，我瞎说的，别当真…<font><br>\n",
    "<font color=red>关键是Cabin这鬼属性，应该算作类目型的，本来缺失值就多，还如此不集中，注定是个棘手货…第一感觉，这玩意儿如果直接按照类目特征处理的话，太散了，估计每个因子化后的特征都拿不到什么权重。加上有那么多缺失值，要不我们先把Cabin缺失与否作为条件(虽然这部分信息缺失可能并非未登记，maybe只是丢失了而已，所以这样做未必妥当)，先在有无Cabin信息这个粗粒度上看看Survived的情况好了。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2109c6e5f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAESCAYAAADnvkIDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x210ae2d6668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cabin的值计数太分散了，绝大多数Cabin值只出现一次。感觉上作为类目，加入特征未必会有效\n",
    "#那我们一起看看这个值的有无，对于survival的分布状况，影响如何吧\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_cabin = data_train.Survived[pd.notnull(data_train.Cabin)].value_counts()\n",
    "Survived_nocabin = data_train.Survived[pd.isnull(data_train.Cabin)].value_counts()\n",
    "df=pd.DataFrame({u'有':Survived_cabin, u'无':Survived_nocabin}).transpose()\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"按Cabin有无看获救情况\")\n",
    "plt.xlabel(u\"Cabin有无\") \n",
    "plt.ylabel(u\"人数\")\n",
    "plt.show()\n",
    "\n",
    "#似乎有cabin记录的乘客survival比例稍高，那先试试把这个值分为两类，有cabin值/无cabin值，一会儿加到类别特征好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/15.png?imageView/2/w/400/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>有Cabin记录的似乎获救概率稍高一些，先这么着放一放吧。<font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>先从最突出的数据属性开始吧，对，Cabin和Age，有丢失数据实在是对下一步工作影响太大。<font><br>\n",
    "\n",
    "<font color=red>先说Cabin，暂时我们就按照刚才说的，按Cabin有无数据，将这个属性处理成Yes和No两种类型吧。<font><br>\n",
    "\n",
    "<font color=red>再说Age：<font><br>\n",
    "\n",
    "<font color=red>通常遇到缺值的情况，我们会有几种常见的处理方式<font><br>\n",
    "\n",
    "1. <font color=red>如果缺值的样本占总数比例极高，我们可能就直接舍弃了，作为特征加入的话，可能反倒带入noise，影响最后的结果了<font><br>\n",
    "2. <font color=red>如果缺值的样本适中，而该属性非连续值特征属性(比如说类目属性)，那就把NaN作为一个新类别，加到类别特征中<font><br>\n",
    "3. <font color=red>如果缺值的样本适中，而该属性为连续值特征属性，有时候我们会考虑给定一个step(比如这里的age，我们可以考虑每隔2/3岁为一个步长)，然后把它离散化，之后把NaN作为一个type加到属性类目中。<font><br>\n",
    "4. <font color=red>有些情况下，缺失的值个数并不是特别多，那我们也可以试着根据已有的值，拟合一下数据，补充上。<font><br>\n",
    "<font color=red>本例中，后两种处理方式应该都是可行的，我们先试试拟合补全吧(虽然说没有特别多的背景可供我们拟合，这不一定是一个多么好的选择)<font><br>\n",
    "\n",
    "<font color=red>我们这里用scikit-learn中的RandomForest来拟合一下缺失的年龄数据<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Yes</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>23.838953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hewlett, Mrs. (Mary D Kingcome)</td>\n",
       "      <td>female</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248706</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>32.066493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Mrs. Julius (Emelia Maria Vande...</td>\n",
       "      <td>female</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>345763</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Fynney, Mr. Joseph J</td>\n",
       "      <td>male</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239865</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beesley, Mr. Lawrence</td>\n",
       "      <td>male</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248698</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>McGowan, Miss. Anna \"Annie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330923</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sloper, Mr. William Thompson</td>\n",
       "      <td>male</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113788</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Miss. Torborg Danira</td>\n",
       "      <td>female</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347077</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2631</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>22.380113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330959</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Todoroff, Mr. Lalio</td>\n",
       "      <td>male</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349216</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Giles, Mr. Frederick Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28134</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Swift, Mrs. Frederick Joel (Margaret Welles Ba...</td>\n",
       "      <td>female</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17466</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Dorothy Edith \"Dolly\"</td>\n",
       "      <td>female</td>\n",
       "      <td>10.869867</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Gill, Mr. John William</td>\n",
       "      <td>male</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233866</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bystrom, Mrs. (Karolina)</td>\n",
       "      <td>female</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236852</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Duran y More, Miss. Asuncion</td>\n",
       "      <td>female</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/PARIS 2149</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Roebling, Mr. Washington Augustus II</td>\n",
       "      <td>male</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17590</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>25.977889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345777</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Master. Harold Theodor</td>\n",
       "      <td>male</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Balkic, Mr. Cerin</td>\n",
       "      <td>male</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349248</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mrs. Richard Leonard (Sallie Monypeny)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Carlsson, Mr. Frans Olof</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>695</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Cruyssen, Mr. Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345765</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Abelson, Mrs. Samuel (Hannah Wizosky)</td>\n",
       "      <td>female</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>P/PP 3381</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Najib, Miss. Adele Kiamie \"Jane\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2667</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Gustafsson, Mr. Alfred Ossian</td>\n",
       "      <td>male</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7534</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Petroff, Mr. Nedelio</td>\n",
       "      <td>male</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349212</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Laleff, Mr. Kristo</td>\n",
       "      <td>male</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349217</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)</td>\n",
       "      <td>female</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11767</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>Yes</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Shelley, Mrs. William (Imanita Parrish Hall)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>230433</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Markun, Mr. Johann</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349257</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dahlberg, Miss. Gerda Ulrika</td>\n",
       "      <td>female</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7552</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Banfield, Mr. Frederick James</td>\n",
       "      <td>male</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A./SOTON 34068</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sutehall, Mr. Henry Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 392076</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Mrs. William (Margaret Norton)</td>\n",
       "      <td>female</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>16.193950</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "5              6         0       3   \n",
       "6              7         0       1   \n",
       "7              8         0       3   \n",
       "8              9         1       3   \n",
       "9             10         1       2   \n",
       "10            11         1       3   \n",
       "11            12         1       1   \n",
       "12            13         0       3   \n",
       "13            14         0       3   \n",
       "14            15         0       3   \n",
       "15            16         1       2   \n",
       "16            17         0       3   \n",
       "17            18         1       2   \n",
       "18            19         0       3   \n",
       "19            20         1       3   \n",
       "20            21         0       2   \n",
       "21            22         1       2   \n",
       "22            23         1       3   \n",
       "23            24         1       1   \n",
       "24            25         0       3   \n",
       "25            26         1       3   \n",
       "26            27         0       3   \n",
       "27            28         0       1   \n",
       "28            29         1       3   \n",
       "29            30         0       3   \n",
       "..           ...       ...     ...   \n",
       "861          862         0       2   \n",
       "862          863         1       1   \n",
       "863          864         0       3   \n",
       "864          865         0       2   \n",
       "865          866         1       2   \n",
       "866          867         1       2   \n",
       "867          868         0       1   \n",
       "868          869         0       3   \n",
       "869          870         1       3   \n",
       "870          871         0       3   \n",
       "871          872         1       1   \n",
       "872          873         0       1   \n",
       "873          874         0       3   \n",
       "874          875         1       2   \n",
       "875          876         1       3   \n",
       "876          877         0       3   \n",
       "877          878         0       3   \n",
       "878          879         0       3   \n",
       "879          880         1       1   \n",
       "880          881         1       2   \n",
       "881          882         0       3   \n",
       "882          883         0       3   \n",
       "883          884         0       2   \n",
       "884          885         0       3   \n",
       "885          886         0       3   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex        Age  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.000000   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.000000   \n",
       "2                               Heikkinen, Miss. Laina  female  26.000000   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.000000   \n",
       "4                             Allen, Mr. William Henry    male  35.000000   \n",
       "5                                     Moran, Mr. James    male  23.838953   \n",
       "6                              McCarthy, Mr. Timothy J    male  54.000000   \n",
       "7                       Palsson, Master. Gosta Leonard    male   2.000000   \n",
       "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.000000   \n",
       "9                  Nasser, Mrs. Nicholas (Adele Achem)  female  14.000000   \n",
       "10                     Sandstrom, Miss. Marguerite Rut  female   4.000000   \n",
       "11                            Bonnell, Miss. Elizabeth  female  58.000000   \n",
       "12                      Saundercock, Mr. William Henry    male  20.000000   \n",
       "13                         Andersson, Mr. Anders Johan    male  39.000000   \n",
       "14                Vestrom, Miss. Hulda Amanda Adolfina  female  14.000000   \n",
       "15                    Hewlett, Mrs. (Mary D Kingcome)   female  55.000000   \n",
       "16                                Rice, Master. Eugene    male   2.000000   \n",
       "17                        Williams, Mr. Charles Eugene    male  32.066493   \n",
       "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.000000   \n",
       "19                             Masselmani, Mrs. Fatima  female  29.518205   \n",
       "20                                Fynney, Mr. Joseph J    male  35.000000   \n",
       "21                               Beesley, Mr. Lawrence    male  34.000000   \n",
       "22                         McGowan, Miss. Anna \"Annie\"  female  15.000000   \n",
       "23                        Sloper, Mr. William Thompson    male  28.000000   \n",
       "24                       Palsson, Miss. Torborg Danira  female   8.000000   \n",
       "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.000000   \n",
       "26                             Emir, Mr. Farred Chehab    male  29.518205   \n",
       "27                      Fortune, Mr. Charles Alexander    male  19.000000   \n",
       "28                       O'Dwyer, Miss. Ellen \"Nellie\"  female  22.380113   \n",
       "29                                 Todoroff, Mr. Lalio    male  27.947206   \n",
       "..                                                 ...     ...        ...   \n",
       "861                        Giles, Mr. Frederick Edward    male  21.000000   \n",
       "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...  female  48.000000   \n",
       "863                  Sage, Miss. Dorothy Edith \"Dolly\"  female  10.869867   \n",
       "864                             Gill, Mr. John William    male  24.000000   \n",
       "865                           Bystrom, Mrs. (Karolina)  female  42.000000   \n",
       "866                       Duran y More, Miss. Asuncion  female  27.000000   \n",
       "867               Roebling, Mr. Washington Augustus II    male  31.000000   \n",
       "868                        van Melkebeke, Mr. Philemon    male  25.977889   \n",
       "869                    Johnson, Master. Harold Theodor    male   4.000000   \n",
       "870                                  Balkic, Mr. Cerin    male  26.000000   \n",
       "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)  female  47.000000   \n",
       "872                           Carlsson, Mr. Frans Olof    male  33.000000   \n",
       "873                        Vander Cruyssen, Mr. Victor    male  47.000000   \n",
       "874              Abelson, Mrs. Samuel (Hannah Wizosky)  female  28.000000   \n",
       "875                   Najib, Miss. Adele Kiamie \"Jane\"  female  15.000000   \n",
       "876                      Gustafsson, Mr. Alfred Ossian    male  20.000000   \n",
       "877                               Petroff, Mr. Nedelio    male  19.000000   \n",
       "878                                 Laleff, Mr. Kristo    male  27.947206   \n",
       "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)  female  56.000000   \n",
       "880       Shelley, Mrs. William (Imanita Parrish Hall)  female  25.000000   \n",
       "881                                 Markun, Mr. Johann    male  33.000000   \n",
       "882                       Dahlberg, Miss. Gerda Ulrika  female  22.000000   \n",
       "883                      Banfield, Mr. Frederick James    male  28.000000   \n",
       "884                             Sutehall, Mr. Henry Jr    male  25.000000   \n",
       "885               Rice, Mrs. William (Margaret Norton)  female  39.000000   \n",
       "886                              Montvila, Rev. Juozas    male  27.000000   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.000000   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female  16.193950   \n",
       "889                              Behr, Mr. Karl Howell    male  26.000000   \n",
       "890                                Dooley, Mr. Patrick    male  32.000000   \n",
       "\n",
       "     SibSp  Parch            Ticket      Fare Cabin Embarked  \n",
       "0        1      0         A/5 21171    7.2500    No        S  \n",
       "1        1      0          PC 17599   71.2833   Yes        C  \n",
       "2        0      0  STON/O2. 3101282    7.9250    No        S  \n",
       "3        1      0            113803   53.1000   Yes        S  \n",
       "4        0      0            373450    8.0500    No        S  \n",
       "5        0      0            330877    8.4583    No        Q  \n",
       "6        0      0             17463   51.8625   Yes        S  \n",
       "7        3      1            349909   21.0750    No        S  \n",
       "8        0      2            347742   11.1333    No        S  \n",
       "9        1      0            237736   30.0708    No        C  \n",
       "10       1      1           PP 9549   16.7000   Yes        S  \n",
       "11       0      0            113783   26.5500   Yes        S  \n",
       "12       0      0         A/5. 2151    8.0500    No        S  \n",
       "13       1      5            347082   31.2750    No        S  \n",
       "14       0      0            350406    7.8542    No        S  \n",
       "15       0      0            248706   16.0000    No        S  \n",
       "16       4      1            382652   29.1250    No        Q  \n",
       "17       0      0            244373   13.0000    No        S  \n",
       "18       1      0            345763   18.0000    No        S  \n",
       "19       0      0              2649    7.2250    No        C  \n",
       "20       0      0            239865   26.0000    No        S  \n",
       "21       0      0            248698   13.0000   Yes        S  \n",
       "22       0      0            330923    8.0292    No        Q  \n",
       "23       0      0            113788   35.5000   Yes        S  \n",
       "24       3      1            349909   21.0750    No        S  \n",
       "25       1      5            347077   31.3875    No        S  \n",
       "26       0      0              2631    7.2250    No        C  \n",
       "27       3      2             19950  263.0000   Yes        S  \n",
       "28       0      0            330959    7.8792    No        Q  \n",
       "29       0      0            349216    7.8958    No        S  \n",
       "..     ...    ...               ...       ...   ...      ...  \n",
       "861      1      0             28134   11.5000    No        S  \n",
       "862      0      0             17466   25.9292   Yes        S  \n",
       "863      8      2          CA. 2343   69.5500    No        S  \n",
       "864      0      0            233866   13.0000    No        S  \n",
       "865      0      0            236852   13.0000    No        S  \n",
       "866      1      0     SC/PARIS 2149   13.8583    No        C  \n",
       "867      0      0          PC 17590   50.4958   Yes        S  \n",
       "868      0      0            345777    9.5000    No        S  \n",
       "869      1      1            347742   11.1333    No        S  \n",
       "870      0      0            349248    7.8958    No        S  \n",
       "871      1      1             11751   52.5542   Yes        S  \n",
       "872      0      0               695    5.0000   Yes        S  \n",
       "873      0      0            345765    9.0000    No        S  \n",
       "874      1      0         P/PP 3381   24.0000    No        C  \n",
       "875      0      0              2667    7.2250    No        C  \n",
       "876      0      0              7534    9.8458    No        S  \n",
       "877      0      0            349212    7.8958    No        S  \n",
       "878      0      0            349217    7.8958    No        S  \n",
       "879      0      1             11767   83.1583   Yes        C  \n",
       "880      0      1            230433   26.0000    No        S  \n",
       "881      0      0            349257    7.8958    No        S  \n",
       "882      0      0              7552   10.5167    No        S  \n",
       "883      0      0  C.A./SOTON 34068   10.5000    No        S  \n",
       "884      0      0   SOTON/OQ 392076    7.0500    No        S  \n",
       "885      0      5            382652   29.1250    No        Q  \n",
       "886      0      0            211536   13.0000    No        S  \n",
       "887      0      0            112053   30.0000   Yes        S  \n",
       "888      1      2        W./C. 6607   23.4500    No        S  \n",
       "889      0      0            111369   30.0000   Yes        C  \n",
       "890      0      0            370376    7.7500    No        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "### 使用 RandomForestClassifier 填补缺失的年龄属性\n",
    "def set_missing_ages(df):\n",
    "    \n",
    "    # 把已有的数值型特征取出来丢进Random Forest Regressor中\n",
    "    age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "\n",
    "    # 乘客分成已知年龄和未知年龄两部分\n",
    "    known_age = age_df[age_df.Age.notnull()].as_matrix()\n",
    "    unknown_age = age_df[age_df.Age.isnull()].as_matrix()\n",
    "\n",
    "    # y即目标年龄\n",
    "    y = known_age[:, 0]\n",
    "\n",
    "    # X即特征属性值\n",
    "    X = known_age[:, 1:]\n",
    "\n",
    "    # fit到RandomForestRegressor之中\n",
    "    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)\n",
    "    rfr.fit(X, y)\n",
    "    \n",
    "    # 用得到的模型进行未知年龄结果预测\n",
    "    predictedAges = rfr.predict(unknown_age[:, 1::])\n",
    "    \n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges\n",
    "    \n",
    "    return df, rfr\n",
    "\n",
    "def set_Cabin_type(df):\n",
    "    df.loc[ (df.Cabin.notnull()), 'Cabin' ] = \"Yes\"\n",
    "    df.loc[ (df.Cabin.isnull()), 'Cabin' ] = \"No\"\n",
    "    return df\n",
    "\n",
    "data_train, rfr = set_missing_ages(data_train)\n",
    "data_train = set_Cabin_type(data_train)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>因为逻辑回归建模时，需要输入的特征都是数值型特征，我们通常会先对类目型的特征因子化/one-hot编码。 <font><br>\n",
    "<font color=red>什么叫做因子化/one-hot编码？举个例子：<font><br>\n",
    "\n",
    "<font color=red>以Embarked为例，原本一个属性维度，因为其取值可以是[‘S’,’C’,’Q‘]，而将其平展开为’Embarked_C’,’Embarked_S’, ‘Embarked_Q’三个属性<font><br>\n",
    "\n",
    "* <font color=red>原本Embarked取值为S的，在此处的”Embarked_S”下取值为1，在’Embarked_C’, ‘Embarked_Q’下取值为0<font><br>\n",
    "* <font color=red>原本Embarked取值为C的，在此处的”Embarked_C”下取值为1，在’Embarked_S’, ‘Embarked_Q’下取值为0<font><br>\n",
    "* <font color=red>原本Embarked取值为Q的，在此处的”Embarked_Q”下取值为1，在’Embarked_C’, ‘Embarked_S’下取值为0<font><br>\n",
    "\n",
    "<font color=red>我们使用pandas的”get_dummies”来完成这个工作，并拼接在原来的”data_train”之上，如下所示。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin_No</th>\n",
       "      <th>Cabin_Yes</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23.838953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>32.066493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>22.380113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>10.869867</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>1</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>25.977889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>1</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>1</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>16.193950</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived        Age  SibSp  Parch      Fare  Cabin_No  \\\n",
       "0              1         0  22.000000      1      0    7.2500         1   \n",
       "1              2         1  38.000000      1      0   71.2833         0   \n",
       "2              3         1  26.000000      0      0    7.9250         1   \n",
       "3              4         1  35.000000      1      0   53.1000         0   \n",
       "4              5         0  35.000000      0      0    8.0500         1   \n",
       "5              6         0  23.838953      0      0    8.4583         1   \n",
       "6              7         0  54.000000      0      0   51.8625         0   \n",
       "7              8         0   2.000000      3      1   21.0750         1   \n",
       "8              9         1  27.000000      0      2   11.1333         1   \n",
       "9             10         1  14.000000      1      0   30.0708         1   \n",
       "10            11         1   4.000000      1      1   16.7000         0   \n",
       "11            12         1  58.000000      0      0   26.5500         0   \n",
       "12            13         0  20.000000      0      0    8.0500         1   \n",
       "13            14         0  39.000000      1      5   31.2750         1   \n",
       "14            15         0  14.000000      0      0    7.8542         1   \n",
       "15            16         1  55.000000      0      0   16.0000         1   \n",
       "16            17         0   2.000000      4      1   29.1250         1   \n",
       "17            18         1  32.066493      0      0   13.0000         1   \n",
       "18            19         0  31.000000      1      0   18.0000         1   \n",
       "19            20         1  29.518205      0      0    7.2250         1   \n",
       "20            21         0  35.000000      0      0   26.0000         1   \n",
       "21            22         1  34.000000      0      0   13.0000         0   \n",
       "22            23         1  15.000000      0      0    8.0292         1   \n",
       "23            24         1  28.000000      0      0   35.5000         0   \n",
       "24            25         0   8.000000      3      1   21.0750         1   \n",
       "25            26         1  38.000000      1      5   31.3875         1   \n",
       "26            27         0  29.518205      0      0    7.2250         1   \n",
       "27            28         0  19.000000      3      2  263.0000         0   \n",
       "28            29         1  22.380113      0      0    7.8792         1   \n",
       "29            30         0  27.947206      0      0    7.8958         1   \n",
       "..           ...       ...        ...    ...    ...       ...       ...   \n",
       "861          862         0  21.000000      1      0   11.5000         1   \n",
       "862          863         1  48.000000      0      0   25.9292         0   \n",
       "863          864         0  10.869867      8      2   69.5500         1   \n",
       "864          865         0  24.000000      0      0   13.0000         1   \n",
       "865          866         1  42.000000      0      0   13.0000         1   \n",
       "866          867         1  27.000000      1      0   13.8583         1   \n",
       "867          868         0  31.000000      0      0   50.4958         0   \n",
       "868          869         0  25.977889      0      0    9.5000         1   \n",
       "869          870         1   4.000000      1      1   11.1333         1   \n",
       "870          871         0  26.000000      0      0    7.8958         1   \n",
       "871          872         1  47.000000      1      1   52.5542         0   \n",
       "872          873         0  33.000000      0      0    5.0000         0   \n",
       "873          874         0  47.000000      0      0    9.0000         1   \n",
       "874          875         1  28.000000      1      0   24.0000         1   \n",
       "875          876         1  15.000000      0      0    7.2250         1   \n",
       "876          877         0  20.000000      0      0    9.8458         1   \n",
       "877          878         0  19.000000      0      0    7.8958         1   \n",
       "878          879         0  27.947206      0      0    7.8958         1   \n",
       "879          880         1  56.000000      0      1   83.1583         0   \n",
       "880          881         1  25.000000      0      1   26.0000         1   \n",
       "881          882         0  33.000000      0      0    7.8958         1   \n",
       "882          883         0  22.000000      0      0   10.5167         1   \n",
       "883          884         0  28.000000      0      0   10.5000         1   \n",
       "884          885         0  25.000000      0      0    7.0500         1   \n",
       "885          886         0  39.000000      0      5   29.1250         1   \n",
       "886          887         0  27.000000      0      0   13.0000         1   \n",
       "887          888         1  19.000000      0      0   30.0000         0   \n",
       "888          889         0  16.193950      1      2   23.4500         1   \n",
       "889          890         1  26.000000      0      0   30.0000         0   \n",
       "890          891         0  32.000000      0      0    7.7500         1   \n",
       "\n",
       "     Cabin_Yes  Embarked_C  Embarked_Q  Embarked_S  Sex_female  Sex_male  \\\n",
       "0            0           0           0           1           0         1   \n",
       "1            1           1           0           0           1         0   \n",
       "2            0           0           0           1           1         0   \n",
       "3            1           0           0           1           1         0   \n",
       "4            0           0           0           1           0         1   \n",
       "5            0           0           1           0           0         1   \n",
       "6            1           0           0           1           0         1   \n",
       "7            0           0           0           1           0         1   \n",
       "8            0           0           0           1           1         0   \n",
       "9            0           1           0           0           1         0   \n",
       "10           1           0           0           1           1         0   \n",
       "11           1           0           0           1           1         0   \n",
       "12           0           0           0           1           0         1   \n",
       "13           0           0           0           1           0         1   \n",
       "14           0           0           0           1           1         0   \n",
       "15           0           0           0           1           1         0   \n",
       "16           0           0           1           0           0         1   \n",
       "17           0           0           0           1           0         1   \n",
       "18           0           0           0           1           1         0   \n",
       "19           0           1           0           0           1         0   \n",
       "20           0           0           0           1           0         1   \n",
       "21           1           0           0           1           0         1   \n",
       "22           0           0           1           0           1         0   \n",
       "23           1           0           0           1           0         1   \n",
       "24           0           0           0           1           1         0   \n",
       "25           0           0           0           1           1         0   \n",
       "26           0           1           0           0           0         1   \n",
       "27           1           0           0           1           0         1   \n",
       "28           0           0           1           0           1         0   \n",
       "29           0           0           0           1           0         1   \n",
       "..         ...         ...         ...         ...         ...       ...   \n",
       "861          0           0           0           1           0         1   \n",
       "862          1           0           0           1           1         0   \n",
       "863          0           0           0           1           1         0   \n",
       "864          0           0           0           1           0         1   \n",
       "865          0           0           0           1           1         0   \n",
       "866          0           1           0           0           1         0   \n",
       "867          1           0           0           1           0         1   \n",
       "868          0           0           0           1           0         1   \n",
       "869          0           0           0           1           0         1   \n",
       "870          0           0           0           1           0         1   \n",
       "871          1           0           0           1           1         0   \n",
       "872          1           0           0           1           0         1   \n",
       "873          0           0           0           1           0         1   \n",
       "874          0           1           0           0           1         0   \n",
       "875          0           1           0           0           1         0   \n",
       "876          0           0           0           1           0         1   \n",
       "877          0           0           0           1           0         1   \n",
       "878          0           0           0           1           0         1   \n",
       "879          1           1           0           0           1         0   \n",
       "880          0           0           0           1           1         0   \n",
       "881          0           0           0           1           0         1   \n",
       "882          0           0           0           1           1         0   \n",
       "883          0           0           0           1           0         1   \n",
       "884          0           0           0           1           0         1   \n",
       "885          0           0           1           0           1         0   \n",
       "886          0           0           0           1           0         1   \n",
       "887          1           0           0           1           1         0   \n",
       "888          0           0           0           1           1         0   \n",
       "889          1           1           0           0           0         1   \n",
       "890          0           0           1           0           0         1   \n",
       "\n",
       "     Pclass_1  Pclass_2  Pclass_3  \n",
       "0           0         0         1  \n",
       "1           1         0         0  \n",
       "2           0         0         1  \n",
       "3           1         0         0  \n",
       "4           0         0         1  \n",
       "5           0         0         1  \n",
       "6           1         0         0  \n",
       "7           0         0         1  \n",
       "8           0         0         1  \n",
       "9           0         1         0  \n",
       "10          0         0         1  \n",
       "11          1         0         0  \n",
       "12          0         0         1  \n",
       "13          0         0         1  \n",
       "14          0         0         1  \n",
       "15          0         1         0  \n",
       "16          0         0         1  \n",
       "17          0         1         0  \n",
       "18          0         0         1  \n",
       "19          0         0         1  \n",
       "20          0         1         0  \n",
       "21          0         1         0  \n",
       "22          0         0         1  \n",
       "23          1         0         0  \n",
       "24          0         0         1  \n",
       "25          0         0         1  \n",
       "26          0         0         1  \n",
       "27          1         0         0  \n",
       "28          0         0         1  \n",
       "29          0         0         1  \n",
       "..        ...       ...       ...  \n",
       "861         0         1         0  \n",
       "862         1         0         0  \n",
       "863         0         0         1  \n",
       "864         0         1         0  \n",
       "865         0         1         0  \n",
       "866         0         1         0  \n",
       "867         1         0         0  \n",
       "868         0         0         1  \n",
       "869         0         0         1  \n",
       "870         0         0         1  \n",
       "871         1         0         0  \n",
       "872         1         0         0  \n",
       "873         0         0         1  \n",
       "874         0         1         0  \n",
       "875         0         0         1  \n",
       "876         0         0         1  \n",
       "877         0         0         1  \n",
       "878         0         0         1  \n",
       "879         1         0         0  \n",
       "880         0         1         0  \n",
       "881         0         0         1  \n",
       "882         0         0         1  \n",
       "883         0         1         0  \n",
       "884         0         0         1  \n",
       "885         0         0         1  \n",
       "886         0         1         0  \n",
       "887         1         0         0  \n",
       "888         0         0         1  \n",
       "889         1         0         0  \n",
       "890         0         0         1  \n",
       "\n",
       "[891 rows x 16 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为逻辑回归建模时，需要输入的特征都是数值型特征\n",
    "# 我们先对类目型的特征离散/因子化\n",
    "# 以Cabin为例，原本一个属性维度，因为其取值可以是['yes','no']，而将其平展开为'Cabin_yes','Cabin_no'两个属性\n",
    "# 原本Cabin取值为yes的，在此处的'Cabin_yes'下取值为1，在'Cabin_no'下取值为0\n",
    "# 原本Cabin取值为no的，在此处的'Cabin_yes'下取值为0，在'Cabin_no'下取值为1\n",
    "# 我们使用pandas的get_dummies来完成这个工作，并拼接在原来的data_train之上，如下所示\n",
    "dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')\n",
    "\n",
    "dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')\n",
    "\n",
    "dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')\n",
    "\n",
    "dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')\n",
    "\n",
    "df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)\n",
    "df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>我们还得做一些处理，仔细看看Age和Fare两个属性，乘客的数值幅度变化，也忒大了吧！！如果大家了解逻辑回归与梯度下降的话，会知道，各属性值之间scale差距太大，将对收敛速度造成几万点伤害值！甚至不收敛！ (╬▔皿▔)…所以我们先用scikit-learn里面的preprocessing模块对这俩货做一个scaling，所谓scaling，其实就是将一些变化幅度较大的特征化到[-1,1]之内。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 22.          38.          26.          35.          35.          23.83895259\n  54.           2.          27.          14.           4.          58.          20.\n  39.          14.          55.           2.          32.06649305  31.\n  29.51820514  35.          34.          15.          28.           8.          38.\n  29.51820514  19.          22.38011324  27.94720616  40.          36.10804822\n  35.2958243   66.          28.          42.          22.87630686  21.          18.\n  14.          40.          27.          27.94720616   3.          19.\n  30.70572678  33.12898535  35.2958243   23.45968333  18.           7.          21.\n  49.          29.          65.          44.06483036  21.          28.5\n   5.          11.          22.          38.          45.           4.\n  41.20008848  17.09991595  29.          19.          17.          26.          32.\n  16.          21.          26.          32.          25.          27.94720616\n  30.70572678   0.83        30.          22.          29.          23.32262739\n  28.          17.          33.          16.          30.70572678  23.          24.\n  29.          20.          46.          26.          59.          30.70572678\n  71.          23.          34.          34.          28.          27.94720616\n  21.          33.          37.          28.          21.          27.51545426\n  38.          33.55117591  47.          14.5         22.          20.          17.\n  21.          70.5         29.          24.           2.          21.\n  30.70572678  32.5         32.5         54.          12.          35.2958243\n  24.          25.78337698  45.          33.          20.          47.          29.\n  25.          23.          19.          37.          16.          24.\n  25.34409583  22.          24.          19.          18.          19.          27.\n   9.          36.5         42.          51.          22.          55.5\n  40.5         29.78279613  51.          16.          30.          25.52340334\n  10.86986696  44.          40.          26.          17.           1.           9.\n  26.03188214  45.          49.5542756   28.          61.           4.           1.\n  21.          56.          18.           7.30954704  50.          30.          36.\n  10.86986696  31.71894048   9.           1.           4.          46.24976824\n  33.12898535  45.          40.          36.          32.          19.          19.\n   3.          44.          58.          35.2958243   42.          35.2958243\n  24.          28.          10.86986696  34.          45.5         18.           2.\n  32.          26.          16.          40.          24.          35.          22.\n  30.          31.09342985  31.          27.          42.          32.          30.\n  16.          27.          51.          27.94720616  38.          22.          19.\n  20.5         18.           7.30954704  35.          29.          59.           5.\n  24.          31.10838452  44.           8.          19.          33.\n  20.80015413  33.12898535  29.          22.          30.          44.          25.\n  24.          37.          54.          29.78279613  29.          62.          30.\n  41.          29.          34.62028571  30.          35.          50.\n  35.2958243    3.          52.          40.          35.2958243   36.          16.\n  25.          58.          35.          36.87489821  25.          41.          37.\n  35.2958243   63.          45.          35.05181757   7.          35.          65.\n  28.          16.          19.          57.74249226  33.          30.          22.\n  42.          22.          26.          19.          36.          24.          24.\n  41.20008848  23.5          2.          41.57487718  50.          35.2958243\n  23.31368333  19.          42.57451554  30.70572678   0.92        28.57888393\n  17.          30.          30.          24.          18.          26.          28.\n  43.          26.          24.          54.          31.          40.          22.\n  27.          30.          22.          10.86986696  36.          61.          36.\n  31.          16.          23.31368333  45.5         38.          16.\n  31.42587794  27.94720616  29.          41.          45.          45.           2.\n  24.          28.          25.          36.          24.          40.\n  26.68916849   3.          42.          23.          59.96916448  15.          25.\n  29.51820514  28.          22.          38.          22.38011324\n  22.38011324  40.          29.          45.          35.          33.12898535\n  30.          60.          22.87630686  35.2958243   24.          25.          18.\n  19.          22.           3.          31.94479345  22.          27.          20.\n  19.          42.           1.          32.          35.          27.94720616\n  18.           1.          36.          19.89558113  17.          36.          21.\n  28.          23.          24.          22.          31.          46.          23.\n  28.          39.          26.          21.          28.          20.          34.\n  51.           3.          21.           7.30954704  27.94720616\n  20.68131488  33.          35.05181757  44.          30.70572678  34.          18.\n  30.          10.          27.94720616  21.          29.          28.          18.\n  29.78279613  28.          19.          35.2958243   32.          28.\n  26.68916849  42.          17.          50.          14.          21.          24.\n  64.          31.          45.          20.          25.          28.\n  23.47643239   4.          13.          34.           5.          52.          36.\n  24.10899881  30.          49.          30.70572678  29.          65.\n  44.06001827  50.          35.2958243   48.          34.          47.          48.\n  30.70572678  38.          35.05181757  56.          19.89558113   0.75\n  29.78279613  38.          33.          23.          22.          44.65953056\n  34.          29.          22.           2.           9.          35.05181757\n  50.          63.          25.           7.30954704  35.          58.          30.\n   9.          24.10899881  21.          55.          71.          21.\n  18.19479484  54.          25.7554753   25.          24.          17.          21.\n  26.82073281  37.          16.          18.          33.          49.8994093\n  28.          26.          29.          30.70572678  36.          54.          24.\n  47.          34.          34.17374861  36.          32.          30.          22.\n  29.51820514  44.          22.87630686  40.5         50.          38.5155\n  39.          23.           2.          22.87630686  17.           8.01370298\n  30.           7.          45.          30.          24.33021696  22.          36.\n   9.          11.          32.          50.          64.          19.\n  32.79502428  33.           8.          17.          27.          26.3814249\n  22.          22.          62.          48.          38.5155      39.          36.\n  35.2958243   40.          28.          30.70572678  30.70572678  24.          19.\n  29.          22.87630686  32.          62.          53.          36.\n  35.2958243   16.          19.          34.          39.          20.07016773\n  32.          25.          39.          54.          36.          29.04293865\n  18.          47.          60.          22.          30.70572678  35.          52.\n  47.          27.45639428  37.          36.          24.40235514  49.\n  29.51820514  49.          24.          27.94720616  37.51615833  44.          35.\n  36.          30.          27.          22.          40.          39.\n  27.86873699  33.12898535  35.2958243   35.          24.          34.          26.\n   4.          26.          27.          42.          20.          21.          21.\n  61.          57.          21.          26.          19.89558113  80.          51.\n  32.          38.83477484   9.          28.          32.          31.          41.\n  26.68916849  20.          24.           2.          29.56889809   0.75\n  48.          19.          56.          31.10838452  23.          27.94720616\n  18.          21.          26.3814249   18.          24.          27.94720616\n  32.          23.          58.          50.          40.          47.          36.\n  20.          32.          25.          27.51545426  43.          39.08817814\n  40.          31.          70.          31.          35.05181757  18.\n  24.5         18.          43.          36.          23.47643239  27.          20.\n  14.          60.          25.          14.          19.          18.          15.\n  31.           4.          29.56889809  25.          60.          52.          44.\n  19.89558113  49.          42.          18.          35.          18.          25.\n  26.          39.          45.          42.          22.          17.09991595\n  24.          49.8994093   48.          29.          52.          19.          38.\n  27.          27.30887391  33.           6.          17.          34.          50.\n  27.          20.          30.          19.89558113  25.          25.          29.\n  11.          35.05181757  23.          23.          28.5         48.          35.\n  27.94720616  27.94720616  38.42663175  36.          21.          24.          31.\n  70.          16.          30.          19.          31.           4.           6.\n  33.          23.          48.           0.67        28.          18.          34.\n  33.          24.33021696  41.          20.          36.          16.          51.\n  39.17490238  30.5         33.55117591  32.          24.          48.          57.\n  29.51820514  54.          18.          35.2958243    5.          19.89558113\n  43.          13.          17.          29.          16.1939502   25.          25.\n  18.           8.           1.          46.          35.2958243   16.\n  10.86986696  50.38328427  25.          39.          49.          31.          30.\n  30.          34.          31.          11.           0.42        27.          31.\n  39.          18.          39.          33.          26.          39.          35.\n   6.          30.5         38.83477484  23.          31.          43.          10.\n  52.          27.          38.          27.           2.          35.09787898\n  29.56889809   1.          35.2958243   62.          15.           0.83\n  22.87630686  23.          18.          39.          21.          30.70572678\n  32.          50.91095013  20.          16.          30.          34.5\n  17.          42.          10.86986696  35.          28.          43.96476448\n   4.          74.           9.          16.          44.          18.          45.\n  51.          24.          22.87630686  41.          21.          48.\n  10.86986696  24.          42.          27.          31.          25.97788916\n   4.          26.          47.          33.          47.          28.          15.\n  20.          19.          27.94720616  56.          25.          33.          22.\n  28.          25.          39.          27.          19.          16.1939502\n  26.          32.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-24b6b3e8bee5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mage_scale_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age_scaled'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage_scale_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfare_scale_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \"\"\"\n\u001b[0;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m--> 612\u001b[1;33m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 22.          38.          26.          35.          35.          23.83895259\n  54.           2.          27.          14.           4.          58.          20.\n  39.          14.          55.           2.          32.06649305  31.\n  29.51820514  35.          34.          15.          28.           8.          38.\n  29.51820514  19.          22.38011324  27.94720616  40.          36.10804822\n  35.2958243   66.          28.          42.          22.87630686  21.          18.\n  14.          40.          27.          27.94720616   3.          19.\n  30.70572678  33.12898535  35.2958243   23.45968333  18.           7.          21.\n  49.          29.          65.          44.06483036  21.          28.5\n   5.          11.          22.          38.          45.           4.\n  41.20008848  17.09991595  29.          19.          17.          26.          32.\n  16.          21.          26.          32.          25.          27.94720616\n  30.70572678   0.83        30.          22.          29.          23.32262739\n  28.          17.          33.          16.          30.70572678  23.          24.\n  29.          20.          46.          26.          59.          30.70572678\n  71.          23.          34.          34.          28.          27.94720616\n  21.          33.          37.          28.          21.          27.51545426\n  38.          33.55117591  47.          14.5         22.          20.          17.\n  21.          70.5         29.          24.           2.          21.\n  30.70572678  32.5         32.5         54.          12.          35.2958243\n  24.          25.78337698  45.          33.          20.          47.          29.\n  25.          23.          19.          37.          16.          24.\n  25.34409583  22.          24.          19.          18.          19.          27.\n   9.          36.5         42.          51.          22.          55.5\n  40.5         29.78279613  51.          16.          30.          25.52340334\n  10.86986696  44.          40.          26.          17.           1.           9.\n  26.03188214  45.          49.5542756   28.          61.           4.           1.\n  21.          56.          18.           7.30954704  50.          30.          36.\n  10.86986696  31.71894048   9.           1.           4.          46.24976824\n  33.12898535  45.          40.          36.          32.          19.          19.\n   3.          44.          58.          35.2958243   42.          35.2958243\n  24.          28.          10.86986696  34.          45.5         18.           2.\n  32.          26.          16.          40.          24.          35.          22.\n  30.          31.09342985  31.          27.          42.          32.          30.\n  16.          27.          51.          27.94720616  38.          22.          19.\n  20.5         18.           7.30954704  35.          29.          59.           5.\n  24.          31.10838452  44.           8.          19.          33.\n  20.80015413  33.12898535  29.          22.          30.          44.          25.\n  24.          37.          54.          29.78279613  29.          62.          30.\n  41.          29.          34.62028571  30.          35.          50.\n  35.2958243    3.          52.          40.          35.2958243   36.          16.\n  25.          58.          35.          36.87489821  25.          41.          37.\n  35.2958243   63.          45.          35.05181757   7.          35.          65.\n  28.          16.          19.          57.74249226  33.          30.          22.\n  42.          22.          26.          19.          36.          24.          24.\n  41.20008848  23.5          2.          41.57487718  50.          35.2958243\n  23.31368333  19.          42.57451554  30.70572678   0.92        28.57888393\n  17.          30.          30.          24.          18.          26.          28.\n  43.          26.          24.          54.          31.          40.          22.\n  27.          30.          22.          10.86986696  36.          61.          36.\n  31.          16.          23.31368333  45.5         38.          16.\n  31.42587794  27.94720616  29.          41.          45.          45.           2.\n  24.          28.          25.          36.          24.          40.\n  26.68916849   3.          42.          23.          59.96916448  15.          25.\n  29.51820514  28.          22.          38.          22.38011324\n  22.38011324  40.          29.          45.          35.          33.12898535\n  30.          60.          22.87630686  35.2958243   24.          25.          18.\n  19.          22.           3.          31.94479345  22.          27.          20.\n  19.          42.           1.          32.          35.          27.94720616\n  18.           1.          36.          19.89558113  17.          36.          21.\n  28.          23.          24.          22.          31.          46.          23.\n  28.          39.          26.          21.          28.          20.          34.\n  51.           3.          21.           7.30954704  27.94720616\n  20.68131488  33.          35.05181757  44.          30.70572678  34.          18.\n  30.          10.          27.94720616  21.          29.          28.          18.\n  29.78279613  28.          19.          35.2958243   32.          28.\n  26.68916849  42.          17.          50.          14.          21.          24.\n  64.          31.          45.          20.          25.          28.\n  23.47643239   4.          13.          34.           5.          52.          36.\n  24.10899881  30.          49.          30.70572678  29.          65.\n  44.06001827  50.          35.2958243   48.          34.          47.          48.\n  30.70572678  38.          35.05181757  56.          19.89558113   0.75\n  29.78279613  38.          33.          23.          22.          44.65953056\n  34.          29.          22.           2.           9.          35.05181757\n  50.          63.          25.           7.30954704  35.          58.          30.\n   9.          24.10899881  21.          55.          71.          21.\n  18.19479484  54.          25.7554753   25.          24.          17.          21.\n  26.82073281  37.          16.          18.          33.          49.8994093\n  28.          26.          29.          30.70572678  36.          54.          24.\n  47.          34.          34.17374861  36.          32.          30.          22.\n  29.51820514  44.          22.87630686  40.5         50.          38.5155\n  39.          23.           2.          22.87630686  17.           8.01370298\n  30.           7.          45.          30.          24.33021696  22.          36.\n   9.          11.          32.          50.          64.          19.\n  32.79502428  33.           8.          17.          27.          26.3814249\n  22.          22.          62.          48.          38.5155      39.          36.\n  35.2958243   40.          28.          30.70572678  30.70572678  24.          19.\n  29.          22.87630686  32.          62.          53.          36.\n  35.2958243   16.          19.          34.          39.          20.07016773\n  32.          25.          39.          54.          36.          29.04293865\n  18.          47.          60.          22.          30.70572678  35.          52.\n  47.          27.45639428  37.          36.          24.40235514  49.\n  29.51820514  49.          24.          27.94720616  37.51615833  44.          35.\n  36.          30.          27.          22.          40.          39.\n  27.86873699  33.12898535  35.2958243   35.          24.          34.          26.\n   4.          26.          27.          42.          20.          21.          21.\n  61.          57.          21.          26.          19.89558113  80.          51.\n  32.          38.83477484   9.          28.          32.          31.          41.\n  26.68916849  20.          24.           2.          29.56889809   0.75\n  48.          19.          56.          31.10838452  23.          27.94720616\n  18.          21.          26.3814249   18.          24.          27.94720616\n  32.          23.          58.          50.          40.          47.          36.\n  20.          32.          25.          27.51545426  43.          39.08817814\n  40.          31.          70.          31.          35.05181757  18.\n  24.5         18.          43.          36.          23.47643239  27.          20.\n  14.          60.          25.          14.          19.          18.          15.\n  31.           4.          29.56889809  25.          60.          52.          44.\n  19.89558113  49.          42.          18.          35.          18.          25.\n  26.          39.          45.          42.          22.          17.09991595\n  24.          49.8994093   48.          29.          52.          19.          38.\n  27.          27.30887391  33.           6.          17.          34.          50.\n  27.          20.          30.          19.89558113  25.          25.          29.\n  11.          35.05181757  23.          23.          28.5         48.          35.\n  27.94720616  27.94720616  38.42663175  36.          21.          24.          31.\n  70.          16.          30.          19.          31.           4.           6.\n  33.          23.          48.           0.67        28.          18.          34.\n  33.          24.33021696  41.          20.          36.          16.          51.\n  39.17490238  30.5         33.55117591  32.          24.          48.          57.\n  29.51820514  54.          18.          35.2958243    5.          19.89558113\n  43.          13.          17.          29.          16.1939502   25.          25.\n  18.           8.           1.          46.          35.2958243   16.\n  10.86986696  50.38328427  25.          39.          49.          31.          30.\n  30.          34.          31.          11.           0.42        27.          31.\n  39.          18.          39.          33.          26.          39.          35.\n   6.          30.5         38.83477484  23.          31.          43.          10.\n  52.          27.          38.          27.           2.          35.09787898\n  29.56889809   1.          35.2958243   62.          15.           0.83\n  22.87630686  23.          18.          39.          21.          30.70572678\n  32.          50.91095013  20.          16.          30.          34.5\n  17.          42.          10.86986696  35.          28.          43.96476448\n   4.          74.           9.          16.          44.          18.          45.\n  51.          24.          22.87630686  41.          21.          48.\n  10.86986696  24.          42.          27.          31.          25.97788916\n   4.          26.          47.          33.          47.          28.          15.\n  20.          19.          27.94720616  56.          25.          33.          22.\n  28.          25.          39.          27.          19.          16.1939502\n  26.          32.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# 接下来我们要接着做一些数据预处理的工作，比如scaling，将一些变化幅度较大的特征化到[-1,1]之内\n",
    "# 这样可以加速logistic regression的收敛\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "age_scale_param = scaler.fit(df['Age'])\n",
    "df['Age_scaled'] = scaler.fit_transform(df['Age'].values.reshape(-1,1), age_scale_param)\n",
    "fare_scale_param = scaler.fit(df['Fare'])\n",
    "df['Fare_scaled'] = scaler.fit_transform(df['Fare'].values.reshape(-1,1), fare_scale_param)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>我们把需要的feature字段取出来，转成numpy格式，使用scikit-learn中的LogisticRegression建模。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们把需要的feature字段取出来，转成numpy格式，使用scikit-learn中的LogisticRegression建模\n",
    "from sklearn import linear_model\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到RandomForestRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(X, y)\n",
    "    \n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来咱们对训练集和测试集做一样的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.loc[ (data_test.Fare.isnull()), 'Fare' ] = 0\n",
    "# 接着我们对test_data做和train_data中一致的特征变换\n",
    "# 首先用同样的RandomForestRegressor模型填上丢失的年龄\n",
    "tmp_df = data_test[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "null_age = tmp_df[data_test.Age.isnull()].as_matrix()\n",
    "# 根据特征属性X预测年龄并补上\n",
    "X = null_age[:, 1:]\n",
    "predictedAges = rfr.predict(X)\n",
    "data_test.loc[ (data_test.Age.isnull()), 'Age' ] = predictedAges\n",
    "\n",
    "data_test = set_Cabin_type(data_test)\n",
    "dummies_Cabin = pd.get_dummies(data_test['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_test['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_test['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_test['Pclass'], prefix= 'Pclass')\n",
    "\n",
    "\n",
    "df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)\n",
    "df_test.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "df_test['Age_scaled'] = scaler.fit_transform(df_test['Age'], age_scale_param)\n",
    "df_test['Fare_scaled'] = scaler.fit_transform(df_test['Fare'], fare_scale_param)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "predictions = clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"logistic_regression_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"logistic_regression_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=red>0.76555，恩，结果还不错。毕竟，这只是我们简单分析过后出的一个baseline系统嘛</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要判定一下当前模型所处状态(欠拟合or过拟合)\n",
    "\n",
    "<font color=red>有一个很可能发生的问题是，我们不断地做feature engineering，产生的特征越来越多，用这些特征去训练模型，会对我们的训练集拟合得越来越好，同时也可能在逐步丧失泛化能力，从而在待预测的数据上，表现不佳，也就是发生过拟合问题。<font><br>\n",
    "\n",
    "<font color=red>从另一个角度上说，如果模型在待预测的数据上表现不佳，除掉上面说的过拟合问题，也有可能是欠拟合问题，也就是说在训练集上，其实拟合的也不是那么好。<font><br>\n",
    "\n",
    "<font color=red>额，这个欠拟合和过拟合怎么解释呢。这么说吧：<font><br>\n",
    "\n",
    "1. <font color=red>过拟合就像是你班那个学数学比较刻板的同学，老师讲过的题目，一字不漏全记下来了，于是老师再出一样的题目，分分钟精确出结果。but数学考试，因为总是碰到新题目，所以成绩不咋地。<font>\n",
    "2. <font color=red>欠拟合就像是，咳咳，和博主level差不多的差生。连老师讲的练习题也记不住，于是连老师出一样题目复习的周测都做不好，考试更是可想而知了。<font>\n",
    "\n",
    "<font color=red>而在机器学习的问题上，对于过拟合和欠拟合两种情形。我们优化的方式是不同的。<font><br>\n",
    "\n",
    "<font color=red>对过拟合而言，通常以下策略对结果优化是有用的：<font><br>\n",
    "\n",
    "* <font color=red>做一下feature selection，挑出较好的feature的subset来做training\n",
    "* <font color=red>提供更多的数据，从而弥补原始数据的bias问题，学习到的model也会更准确\n",
    "\n",
    "<font color=red>而对于欠拟合而言，我们通常需要更多的feature，更复杂的模型来提高准确度。<font><br>\n",
    "\n",
    "<font color=red>著名的learning curve可以帮我们判定我们的模型现在所处的状态。我们以样本数为横坐标，训练和交叉验证集上的错误率作为纵坐标，两种状态分别如下两张图所示：过拟合(overfitting/high variace)，欠拟合(underfitting/high bias)<font><br>\n",
    "\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/high_variance.png?imageView/2/w/400/q/100)\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/high_bias.png?imageView/2/w/400/q/100)\n",
    "\n",
    "<font color=red>著名的learning curve可以帮我们判定我们的模型现在所处的状态。我们以样本数为横坐标，训练和交叉验证集上的错误率作为纵坐标，两种状态分别如下两张图所示：过拟合(overfitting/high variace)，欠拟合(underfitting/high bias)<font><br>\n",
    "\n",
    "<font color=red>我们也可以把错误率替换成准确率(得分)，得到另一种形式的learning curve(sklearn 里面是这么做的)。<font><br>\n",
    "\n",
    "<font color=red>回到我们的问题，我们用scikit-learn里面的learning_curve来帮我们分辨我们模型的状态。举个例子，这里我们一起画一下我们最先得到的baseline model的learning curve。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "# 用sklearn的learning_curve得到training_score和cv_score，使用matplotlib画出learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, \n",
    "                        train_sizes=np.linspace(.05, 1., 20), verbose=0, plot=True):\n",
    "    \"\"\"\n",
    "    画出data在某模型上的learning curve.\n",
    "    参数解释\n",
    "    ----------\n",
    "    estimator : 你用的分类器。\n",
    "    title : 表格的标题。\n",
    "    X : 输入的feature，numpy类型\n",
    "    y : 输入的target vector\n",
    "    ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点\n",
    "    cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)\n",
    "    n_jobs : 并行的的任务数(默认1)\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.xlabel(u\"训练样本数\")\n",
    "        plt.ylabel(u\"得分\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                         alpha=0.1, color=\"b\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                         alpha=0.1, color=\"r\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=u\"训练集上得分\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=u\"交叉验证集上得分\")\n",
    "    \n",
    "        plt.legend(loc=\"best\")\n",
    "        \n",
    "        plt.draw()\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    \n",
    "    midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2\n",
    "    diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])\n",
    "    return midpoint, diff\n",
    "\n",
    "plot_learning_curve(clf, u\"学习曲线\", X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/learning_curve.png?imageView/2/w/600/q/100)\n",
    "<font color=red>在实际数据上看，我们得到的learning curve没有理论推导的那么光滑哈，但是可以大致看出来，训练集和交叉验证集上的得分曲线走势还是符合预期的。<font><br>\n",
    "\n",
    "<font color=red>目前的曲线看来，我们的model并不处于overfitting的状态(overfitting的表现一般是训练集上得分高，而交叉验证集上要低很多，中间的gap比较大)。因此我们可以再做些feature engineering的工作，添加一些新产出的特征或者组合特征到模型中。<font><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>接下来，我们就该看看如何优化baseline系统了<br>\n",
    "我们还有些特征可以再挖掘挖掘<br><br>\n",
    "\n",
    "1. 比如说Name和Ticket两个属性被我们完整舍弃了(好吧，其实是一开始我们对于这种，每一条记录都是一个完全不同的值的属性，并没有很直接的处理方式)<br>\n",
    "2. 比如说，我们想想，年龄的拟合本身也未必是一件非常靠谱的事情<br>\n",
    "3. 另外，以我们的日常经验，小盆友和老人可能得到的照顾会多一些，这样看的话，年龄作为一个连续值，给一个固定的系数，似乎体现不出两头受照顾的实际情况，所以，说不定我们把年龄离散化，按区段分作类别属性会更合适一些<br>\n",
    "\n",
    "那怎么样才知道，哪些地方可以优化，哪些优化的方法是promising的呢？<br>\n",
    "是的<br><br>\n",
    "\n",
    "要做交叉验证(cross validation)!<br>\n",
    "要做交叉验证(cross validation)!<br>\n",
    "要做交叉验证(cross validation)!<br><br>\n",
    "\n",
    "重要的事情说3编！！！<br>\n",
    "因为test.csv里面并没有Survived这个字段(好吧，这是废话，这明明就是我们要预测的结果)，我们无法在这份数据上评定我们算法在该场景下的效果。。。<br>\n",
    "我们通常情况下，这么做cross validation：把train.csv分成两部分，一部分用于训练我们需要的模型，另外一部分数据上看我们预测算法的效果。<br>\n",
    "我们可以用scikit-learn的cross_validation来完成这个工作</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>在此之前，咱们可以看看现在得到的模型的系数，因为系数和它们最终的判定能力强弱是正相关的</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"columns\":list(train_df.columns)[1:], \"coef\":list(clf.coef_.T)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "上面的系数和最后的结果是一个正相关的关系<br>\n",
    "我们先看看那些权重绝对值非常大的feature，在我们的模型上：<br>\n",
    "\n",
    "* Sex属性，如果是female会极大提高最后获救的概率，而male会很大程度拉低这个概率。\n",
    "* Pclass属性，1等舱乘客最后获救的概率会上升，而乘客等级为3会极大地拉低这个概率。\n",
    "* 有Cabin值会很大程度拉升最后获救概率(这里似乎能看到了一点端倪，事实上从最上面的有无Cabin记录的Survived分布图上看出，即使有Cabin记录的乘客也有一部分遇难了，估计这个属性上我们挖掘还不够)\n",
    "* Age是一个负相关，意味着在我们的模型里，年龄越小，越有获救的优先权(还得回原数据看看这个是否合理）\n",
    "* 有一个登船港口S会很大程度拉低获救的概率，另外俩港口压根就没啥作用(这个实际上非常奇怪，因为我们从之前的统计图上并没有看到S港口的获救率非常低，所以也许可以考虑把登船港口这个feature去掉试试)。\n",
    "* 船票Fare有小幅度的正相关(并不意味着这个feature作用不大，有可能是我们细化的程度还不够，举个例子，说不定我们得对它离散化，再分至各个乘客等级上？)\n",
    "\n",
    "噢啦，观察完了，我们现在有一些想法了，但是怎么样才知道，哪些优化的方法是promising的呢？<br>\n",
    "\n",
    "恩，要靠交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "# 简单看看打分情况\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "all_data = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "X = all_data.as_matrix()[:,1:]\n",
    "y = all_data.as_matrix()[:,0]\n",
    "print cross_validation.cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "\n",
    "# 分割数据\n",
    "split_train, split_cv = cross_validation.train_test_split(\u001d",
    "df, test_size=0.3, random_state=0)\n",
    "train_df = split_train.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "# 生成模型\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(train_df.as_matrix()[:,1:], train_df.as_matrix()[:,0])\n",
    "\n",
    "\n",
    "\n",
    "# 对cross validation数据进行预测\n",
    "\n",
    "cv_df = split_cv.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "predictions = clf.predict(cv_df.as_matrix()[:,1:])\n",
    "split_cv[ predictions != cv_df.as_matrix()[:,0] ].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 去除预测错误的case看原始dataframe数据\n",
    "#split_cv['PredictResult'] = predictions\n",
    "origin_data_train = pd.read_csv(\"Train.csv\")\n",
    "bad_cases = origin_data_train.loc[origin_data_train['PassengerId'].isin(split_cv[predictions != cv_df.as_matrix()[:,0]]['PassengerId'].values)]\n",
    "bad_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比bad case，我们仔细看看我们预测错的样本，到底是哪些特征有问题，咱们处理得还不够细？<br>\n",
    "\n",
    "我们随便列一些可能可以做的优化操作：<br>\n",
    "\n",
    "* Age属性不使用现在的拟合方式，而是根据名称中的『Mr』『Mrs』『Miss』等的平均值进行填充。\n",
    "* Age不做成一个连续值属性，而是使用一个步长进行离散化，变成离散的类目feature。\n",
    "* Cabin再细化一些，对于有记录的Cabin属性，我们将其分为前面的字母部分(我猜是位置和船层之类的信息) 和 后面的数字部分(应该是房间号，有意思的事情是，如果你仔细看看原始数据，你会发现，这个值大的情况下，似乎获救的可能性高一些)。\n",
    "* Pclass和Sex俩太重要了，我们试着用它们去组出一个组合属性来试试，这也是另外一种程度的细化。\n",
    "* 单加一个Child字段，Age<=12的，设为1，其余为0(你去看看数据，确实小盆友优先程度很高啊)\n",
    "* 如果名字里面有『Mrs』，而Parch>1的，我们猜测她可能是一个母亲，应该获救的概率也会提高，因此可以多加一个Mother字段，此种情况下设为1，其余情况下设为0\n",
    "* 登船港口可以考虑先去掉试试(Q和C本来就没权重，S有点诡异)\n",
    "* 把堂兄弟/兄妹 和 Parch 还有自己 个数加在一起组一个Family_size字段(考虑到大家族可能对最后的结果有影响)\n",
    "* Name是一个我们一直没有触碰的属性，我们可以做一些简单的处理，比如说男性中带某些字眼的(‘Capt’, ‘Don’, ‘Major’, ‘Sir’)可以统一到一个Title，女性也一样。\n",
    "\n",
    "大家接着往下挖掘，可能还可以想到更多可以细挖的部分。我这里先列这些了，然后我们可以使用手头上的”train_df”和”cv_df”开始试验这些feature engineering的tricks是否有效了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[data_train['Name'].str.contains(\"Major\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"Train.csv\")\n",
    "data_train['Sex_Pclass'] = data_train.Sex + \"_\" + data_train.Pclass.map(str)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "### 使用 RandomForestClassifier 填补缺失的年龄属性\n",
    "def set_missing_ages(df):\n",
    "    \n",
    "    # 把已有的数值型特征取出来丢进Random Forest Regressor中\n",
    "    age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "\n",
    "    # 乘客分成已知年龄和未知年龄两部分\n",
    "    known_age = age_df[age_df.Age.notnull()].as_matrix()\n",
    "    unknown_age = age_df[age_df.Age.isnull()].as_matrix()\n",
    "\n",
    "    # y即目标年龄\n",
    "    y = known_age[:, 0]\n",
    "\n",
    "    # X即特征属性值\n",
    "    X = known_age[:, 1:]\n",
    "\n",
    "    # fit到RandomForestRegressor之中\n",
    "    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)\n",
    "    rfr.fit(X, y)\n",
    "    \n",
    "    # 用得到的模型进行未知年龄结果预测\n",
    "    predictedAges = rfr.predict(unknown_age[:, 1::])\n",
    "    \n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n",
    "    \n",
    "    return df, rfr\n",
    "\n",
    "def set_Cabin_type(df):\n",
    "    df.loc[ (df.Cabin.notnull()), 'Cabin' ] = \"Yes\"\n",
    "    df.loc[ (df.Cabin.isnull()), 'Cabin' ] = \"No\"\n",
    "    return df\n",
    "\n",
    "data_train, rfr = set_missing_ages(data_train)\n",
    "data_train = set_Cabin_type(data_train)\n",
    "\n",
    "dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')\n",
    "dummies_Sex_Pclass = pd.get_dummies(data_train['Sex_Pclass'], prefix= 'Sex_Pclass')\n",
    "\n",
    "\n",
    "df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass, dummies_Sex_Pclass], axis=1)\n",
    "df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Sex_Pclass'], axis=1, inplace=True)\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "age_scale_param = scaler.fit(df['Age'])\n",
    "df['Age_scaled'] = scaler.fit_transform(df['Age'], age_scale_param)\n",
    "fare_scale_param = scaler.fit(df['Fare'])\n",
    "df['Fare_scaled'] = scaler.fit_transform(df['Fare'], fare_scale_param)\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到RandomForestRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(X, y)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.loc[ (data_test.Fare.isnull()), 'Fare' ] = 0\n",
    "data_test['Sex_Pclass'] = data_test.Sex + \"_\" + data_test.Pclass.map(str)\n",
    "# 接着我们对test_data做和train_data中一致的特征变换\n",
    "# 首先用同样的RandomForestRegressor模型填上丢失的年龄\n",
    "tmp_df = data_test[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "null_age = tmp_df[data_test.Age.isnull()].as_matrix()\n",
    "# 根据特征属性X预测年龄并补上\n",
    "X = null_age[:, 1:]\n",
    "predictedAges = rfr.predict(X)\n",
    "data_test.loc[ (data_test.Age.isnull()), 'Age' ] = predictedAges\n",
    "\n",
    "data_test = set_Cabin_type(data_test)\n",
    "dummies_Cabin = pd.get_dummies(data_test['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_test['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_test['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_test['Pclass'], prefix= 'Pclass')\n",
    "dummies_Sex_Pclass = pd.get_dummies(data_test['Sex_Pclass'], prefix= 'Sex_Pclass')\n",
    "\n",
    "\n",
    "df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass, dummies_Sex_Pclass], axis=1)\n",
    "df_test.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Sex_Pclass'], axis=1, inplace=True)\n",
    "df_test['Age_scaled'] = scaler.fit_transform(df_test['Age'], age_scale_param)\n",
    "df_test['Fare_scaled'] = scaler.fit_transform(df_test['Fare'], fare_scale_param)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*')\n",
    "predictions = clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"logistic_regression_predictions2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>一般做到后期，咱们要进行模型优化的方法就是模型融合啦<br>\n",
    "先解释解释啥叫模型融合哈，我们还是举几个例子直观理解一下好了。<br><br>\n",
    "\n",
    "大家都看过知识问答的综艺节目中，求助现场观众时候，让观众投票，最高的答案作为自己的答案的形式吧，每个人都有一个判定结果，最后我们相信答案在大多数人手里。<br>\n",
    "\n",
    "再通俗一点举个例子。你和你班某数学大神关系好，每次作业都『模仿』他的，于是绝大多数情况下，他做对了，你也对了。突然某一天大神脑子犯糊涂，手一抖，写错了一个数，于是…恩，你也只能跟着错了。 <br>\n",
    "我们再来看看另外一个场景，你和你班5个数学大神关系都很好，每次都把他们作业拿过来，对比一下，再『自己做』，那你想想，如果哪天某大神犯糊涂了，写错了，but另外四个写对了啊，那你肯定相信另外4人的是正确答案吧？<br>\n",
    "\n",
    "最简单的模型融合大概就是这么个意思，比如分类问题，当我们手头上有一堆在同一份数据集上训练得到的分类器(比如logistic regression，SVM，KNN，random forest，神经网络)，那我们让他们都分别去做判定，然后对结果做投票统计，取票数最多的结果为最后结果。<br>\n",
    "\n",
    "bingo，问题就这么完美的解决了。<br>\n",
    "\n",
    "模型融合可以比较好地缓解，训练过程中产生的过拟合问题，从而对于结果的准确度提升有一定的帮助。<br>\n",
    "\n",
    "话说回来，回到我们现在的问题。你看，我们现在只讲了logistic regression，如果我们还想用这个融合思想去提高我们的结果，我们该怎么做呢？<br>\n",
    "\n",
    "既然这个时候模型没得选，那咱们就在数据上动动手脚咯。大家想想，如果模型出现过拟合现在，一定是在我们的训练上出现拟合过度造成的对吧。<br>\n",
    "\n",
    "那我们干脆就不要用全部的训练集，每次取训练集的一个subset，做训练，这样，我们虽然用的是同一个机器学习算法，但是得到的模型却是不一样的；同时，因为我们没有任何一份子数据集是全的，因此即使出现过拟合，也是在子训练集上出现过拟合，而不是全体数据上，这样做一个融合，可能对最后的结果有一定的帮助。对，这就是常用的Bagging。<br>\n",
    "\n",
    "我们用scikit-learn里面的Bagging来完成上面的思路，过程非常简单。代码如下：<br><br><font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到BaggingRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "bagging_clf = BaggingRegressor(clf, n_estimators=10, max_samples=0.8, max_features=1.0, bootstrap=True, bootstrap_features=False, n_jobs=-1)\n",
    "bagging_clf.fit(X, y)\n",
    "\n",
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title')\n",
    "predictions = bagging_clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"/Users/MLS/Downloads/logistic_regression_predictions2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>下面是咱们用别的分类器解决这个问题的代码：</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import  DataFrame\n",
    "from patsy import dmatrices\n",
    "import string\n",
    "from operator import itemgetter\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split,StratifiedShuffleSplit,StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "##Read configuration parameters\n",
    "\n",
    "train_file=\"train.csv\"\n",
    "MODEL_PATH=\"./\"\n",
    "test_file=\"test.csv\"\n",
    "SUBMISSION_PATH=\"./\"\n",
    "seed= 0\n",
    "\n",
    "print train_file,seed\n",
    "\n",
    "# 输出得分\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "#清理和处理数据\n",
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if string.find(big_string, substring) != -1:\n",
    "            return substring\n",
    "    print big_string\n",
    "    return np.nan\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "enc=preprocessing.OneHotEncoder()\n",
    "\n",
    "def clean_and_munge_data(df):\n",
    "    #处理缺省值\n",
    "    df.Fare = df.Fare.map(lambda x: np.nan if x==0 else x)\n",
    "    #处理一下名字，生成Title字段\n",
    "    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "                'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n",
    "                'Don', 'Jonkheer']\n",
    "    df['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\n",
    "\n",
    "    #处理特殊的称呼，全处理成mr, mrs, miss, master\n",
    "    def replace_titles(x):\n",
    "        title=x['Title']\n",
    "        if title in ['Mr','Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n",
    "            return 'Mr'\n",
    "        elif title in ['Master']:\n",
    "            return 'Master'\n",
    "        elif title in ['Countess', 'Mme','Mrs']:\n",
    "            return 'Mrs'\n",
    "        elif title in ['Mlle', 'Ms','Miss']:\n",
    "            return 'Miss'\n",
    "        elif title =='Dr':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Mr'\n",
    "            else:\n",
    "                return 'Mrs'\n",
    "        elif title =='':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Master'\n",
    "            else:\n",
    "                return 'Miss'\n",
    "        else:\n",
    "            return title\n",
    "\n",
    "    df['Title']=df.apply(replace_titles, axis=1)\n",
    "\n",
    "    #看看家族是否够大，咳咳\n",
    "    df['Family_Size']=df['SibSp']+df['Parch']\n",
    "    df['Family']=df['SibSp']*df['Parch']\n",
    "\n",
    "\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==1),'Fare'] =np.median(df[df['Pclass'] == 1]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==2),'Fare'] =np.median( df[df['Pclass'] == 2]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==3),'Fare'] = np.median(df[df['Pclass'] == 3]['Fare'].dropna())\n",
    "\n",
    "    df['Gender'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "    df['AgeFill']=df['Age']\n",
    "    mean_ages = np.zeros(4)\n",
    "    mean_ages[0]=np.average(df[df['Title'] == 'Miss']['Age'].dropna())\n",
    "    mean_ages[1]=np.average(df[df['Title'] == 'Mrs']['Age'].dropna())\n",
    "    mean_ages[2]=np.average(df[df['Title'] == 'Mr']['Age'].dropna())\n",
    "    mean_ages[3]=np.average(df[df['Title'] == 'Master']['Age'].dropna())\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Miss') ,'AgeFill'] = mean_ages[0]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mrs') ,'AgeFill'] = mean_ages[1]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mr') ,'AgeFill'] = mean_ages[2]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Master') ,'AgeFill'] = mean_ages[3]\n",
    "\n",
    "    df['AgeCat']=df['AgeFill']\n",
    "    df.loc[ (df.AgeFill<=10) ,'AgeCat'] = 'child'\n",
    "    df.loc[ (df.AgeFill>60),'AgeCat'] = 'aged'\n",
    "    df.loc[ (df.AgeFill>10) & (df.AgeFill <=30) ,'AgeCat'] = 'adult'\n",
    "    df.loc[ (df.AgeFill>30) & (df.AgeFill <=60) ,'AgeCat'] = 'senior'\n",
    "\n",
    "    df.Embarked = df.Embarked.fillna('S')\n",
    "\n",
    "\n",
    "    df.loc[ df.Cabin.isnull()==True,'Cabin'] = 0.5\n",
    "    df.loc[ df.Cabin.isnull()==False,'Cabin'] = 1.5\n",
    "\n",
    "    df['Fare_Per_Person']=df['Fare']/(df['Family_Size']+1)\n",
    "\n",
    "    #Age times class\n",
    "\n",
    "    df['AgeClass']=df['AgeFill']*df['Pclass']\n",
    "    df['ClassFare']=df['Pclass']*df['Fare_Per_Person']\n",
    "\n",
    "\n",
    "    df['HighLow']=df['Pclass']\n",
    "    df.loc[ (df.Fare_Per_Person<8) ,'HighLow'] = 'Low'\n",
    "    df.loc[ (df.Fare_Per_Person>=8) ,'HighLow'] = 'High'\n",
    "\n",
    "\n",
    "\n",
    "    le.fit(df['Sex'] )\n",
    "    x_sex=le.transform(df['Sex'])\n",
    "    df['Sex']=x_sex.astype(np.float)\n",
    "\n",
    "    le.fit( df['Ticket'])\n",
    "    x_Ticket=le.transform( df['Ticket'])\n",
    "    df['Ticket']=x_Ticket.astype(np.float)\n",
    "\n",
    "    le.fit(df['Title'])\n",
    "    x_title=le.transform(df['Title'])\n",
    "    df['Title'] =x_title.astype(np.float)\n",
    "\n",
    "    le.fit(df['HighLow'])\n",
    "    x_hl=le.transform(df['HighLow'])\n",
    "    df['HighLow']=x_hl.astype(np.float)\n",
    "\n",
    "\n",
    "    le.fit(df['AgeCat'])\n",
    "    x_age=le.transform(df['AgeCat'])\n",
    "    df['AgeCat'] =x_age.astype(np.float)\n",
    "\n",
    "    le.fit(df['Embarked'])\n",
    "    x_emb=le.transform(df['Embarked'])\n",
    "    df['Embarked']=x_emb.astype(np.float)\n",
    "\n",
    "    df = df.drop(['PassengerId','Name','Age','Cabin'], axis=1) #remove Name,Age and PassengerId\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "#读取数据\n",
    "traindf=pd.read_csv(train_file)\n",
    "##清洗数据\n",
    "df=clean_and_munge_data(traindf)\n",
    "########################################formula################################\n",
    " \n",
    "formula_ml='Survived~Pclass+C(Title)+Sex+C(AgeCat)+Fare_Per_Person+Fare+Family_Size' \n",
    "\n",
    "y_train, x_train = dmatrices(formula_ml, data=df, return_type='dataframe')\n",
    "y_train = np.asarray(y_train).ravel()\n",
    "print y_train.shape,x_train.shape\n",
    "\n",
    "##选择训练和测试集\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2,random_state=seed)\n",
    "#初始化分类器\n",
    "clf=RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=5, min_samples_split=1,\n",
    "  min_samples_leaf=1, max_features='auto',    bootstrap=False, oob_score=False, n_jobs=1, random_state=seed,\n",
    "  verbose=0)\n",
    "\n",
    "###grid search找到最好的参数\n",
    "param_grid = dict( )\n",
    "##创建分类pipeline\n",
    "pipeline=Pipeline([ ('clf',clf) ])\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=3,scoring='accuracy',\\\n",
    "cv=StratifiedShuffleSplit(Y_train, n_iter=10, test_size=0.2, train_size=None, indices=None, \\\n",
    "random_state=seed, n_iterations=None)).fit(X_train, Y_train)\n",
    "# 对结果打分\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "report(grid_search.grid_scores_)\n",
    " \n",
    "print('-----grid search end------------')\n",
    "print ('on all train set')\n",
    "scores = cross_val_score(grid_search.best_estimator_, x_train, y_train,cv=3,scoring='accuracy')\n",
    "print scores.mean(),scores\n",
    "print ('on test set')\n",
    "scores = cross_val_score(grid_search.best_estimator_, X_test, Y_test,cv=3,scoring='accuracy')\n",
    "print scores.mean(),scores\n",
    "\n",
    "# 对结果打分\n",
    "\n",
    "print(classification_report(Y_train, grid_search.best_estimator_.predict(X_train) ))\n",
    "print('test data')\n",
    "print(classification_report(Y_test, grid_search.best_estimator_.predict(X_test) ))\n",
    "\n",
    "model_file=MODEL_PATH+'model-rf.pkl'\n",
    "joblib.dump(grid_search.best_estimator_, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
